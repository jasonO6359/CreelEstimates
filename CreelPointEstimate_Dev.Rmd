---
title: Creel Point Estimate Analysis
params:
  proj_name: "R5 Steelhead"
  water_body: "Kalama River"
  date_start: "2016-04-30"   
  date_end: "2017-04-30"
  index_count_types: "Bank Anglers,Boat Anglers"
  census_expansion: "indirect"
  model_period: "Week"
  sections: "lut_water_body_location_kalama_steelhead.csv"
output:
  html_document:
    fig_caption: yes
    theme: default
    toc: yes
    toc_depth: 3
    toc_float: yes
    code_folding: hide
---

This file uses a workflow developed by the DFW creel dev crew to summarize and generate creel estimates (angler effort, catch, and catch per unit effort (cpue)) from freshwater monitoring data housed on data.wa.gov, using direct expansion estimators (see Kale Bentley's original creel analysis files, Pollock et al. 1994).

Pieces of the script require user / analysis specific inputs, including the initial parameters in the YAML, specification of closure / river out days in the dates section, and catch groups to retain for estimates.

# setup


```{r setup, message = FALSE, warning = FALSE, include = FALSE}
knitr::opts_chunk$set(message = FALSE)

library(tidyverse)
library(kableExtra)

#base endpoints
dwg_base <- list(
  event = "https://data.wa.gov/resource/ui95-axtn.csv",
  effort = "https://data.wa.gov/resource/h9a6-g38s.csv",
  interview = "https://data.wa.gov/resource/rpax-ahqm.csv",
  catch = "https://data.wa.gov/resource/6y4e-8ftk.csv",
  gear = "https://data.wa.gov/resource/d2ks-afhz.csv"
)

dwg_sent <- list() #will hold full API strings built on above endpoints with params
creel <- list() #will hold resulting data objects

dates_holidays_2015_2030 <- read_lines("input_files/dates_holidays_2015_2030.txt") |> 
  as.Date(format="%Y-%m-%d")

lut <- map(
  list(
    river_loc = "input_files/lut_River.Locations_2019-01-07.csv", # using file with Skagit location
    sections = file.path("input_files", params$sections),
    census_expansion = "input_files/lut_02_Proportional_Expansions_for_Tie_In_Sections_Kalama_Example.csv" 
  ),
  ~readr::read_csv(file.path(.x))
)

# extract the index count types of interest from the YAML and use these to filter the data
index_count_types <- c(str_split(string = params$index_count_types, pattern = ",")[[1]])

# fix angler type in census expansion table for use with vehicle / trailer counts so it joins to TI_expan table 
if(str_detect(index_count_types, c("Vehicle Only", "Trailers Only"))) {
  
lut$census_expansion <- lut$census_expansion |> 
  mutate(
    angler_type = if_else(angler_type == "bank", "total", "boat")
  )
}

# setup folder for model output
model_output <-"model_outputs"          # Location of saved output (summary figures/tables and model results)
# outputs
ifelse(!dir.exists(model_output), {dir.create(model_output); "Output sub-folder created"},"Output sub-folder exists already")
```



# get raw data

The data used are from the `r params$proj_name` project on the `r params$water_body` between `r params$date_start` and `r params$date_end`.

Further development may include interactive control parameter specification via the GUI: [https://bookdown.org/yihui/rmarkdown/params-knit.html#the-interactive-user-interface]

There is also the option to step through multiple pre-defined control parameters:
[https://bookdown.org/yihui/rmarkdown-cookbook/parameterized-reports.html]

## creel events

First, get the creel events of interest by building the Socrata API url string and grabbing the data

```{r get_event, include = FALSE}
dwg_sent$event <- URLencode(
  paste0(dwg_base$event,
         "?$where=project_name in('", params$proj_name, "')",
         " AND water_body in('", str_replace(params$water_body, ",|\\|", "','"), "')",
         " AND event_date between '", params$date_start,
         "T00:00:00' and '", params$date_end,
         "T00:00:00'&$limit=100000"
  )
)

creel$event <- read_csv(dwg_sent$event) |> 
  dplyr::select(creel_event_id, water_body, event_date, tie_in_indicator)

```

Then, get the associated effort and interview data.

```{r pending_vw_changes_for_water_body, include = FALSE}
# #if water_body dropped from event filter...
# #but regardless can/should build creel_event_id condition once and apply twice?
# eff_int_filter <- paste0(
#     "?$where=creel_event_id in('",
#     paste(creel$event$creel_event_id, collapse = "','"), "')",
#     " AND water_body in('", str_replace(params$water_body, ",|\\|", "','"), "')",
#     "&$limit=100000"
#   )

```

## effort counts

```{r get_effort, include = FALSE}
dwg_sent$effort <- URLencode(
  paste0(dwg_base$effort,
         "?$where=creel_event_id in('",
         paste(creel$event$creel_event_id, collapse = "','"),
         "')&$limit=100000"
  )
)

creel$effort <- read_csv(dwg_sent$effort) |> 
  filter(!is.na(count_type))

```

## interviews

```{r get_interview, include = FALSE}
dwg_sent$interview <- URLencode(
  paste0(dwg_base$interview,
         "?$where=creel_event_id in('",
         paste(creel$event$creel_event_id, collapse = "','"),
         "')&$limit=100000"
  )
)

creel$interview <- read_csv(dwg_sent$interview) |> 
  rename(location = interview_location) 
# |> 
  # filter(!(angler_type == "Unk"))

```

## catch data

And finally, the catch data associated with the interviews.

```{r get_catch, include = FALSE}
dwg_sent$catch <- URLencode(
  paste0(dwg_base$catch,
         "?$where=creel_event_id in('",
         paste(creel$event$creel_event_id, collapse = "','"),
         "')&$limit=100000"
  )
)


creel$catch <- read_csv(dwg_sent$catch) 

creel$catch <- creel$catch |> 
  dplyr::select(interview_id, catch_id, species, run, life_stage, fin_mark, fate, fish_count) |> 
  mutate(
    life_stage = replace_na(life_stage, "Adult"), # placeholder pending data corrections in fish apps 
    catch_group = paste(species, life_stage, fin_mark, fate, sep = "_") # fish catch groups to estimate catch of 
  )

```



## sections

Aggregations of `location` units that depend on `r params$section` lookup table.

**NOTE ANY DATA THAT ARE NOT ASSIGNED TO A SECTION WILL BE EXCLUDED**

```{r add_sections, include = FALSE}
creel$effort <- creel$effort |> 
  select(-created_datetime, -modified_datetime) |>
  left_join(
    lut$sections |> select(water_body_desc, location = location_code, section),
    by = c("location")
    ) |> 
  filter(!is.na(section))

creel$interview <- creel$interview |> 
  select(-created_datetime, -modified_datetime,
         -state_residence, -zip_code) |> 
  left_join(
    lut$sections |> select(water_body_desc, location = location_code, section),
    by = c("location")
  ) |> 
  filter(!is.na(section))

```


# prepare data for catch and effort calculations

```{r init_point_estimate_data_prelim, include = FALSE}
#declare an intermediary list

point_estimate_data_prelim <- list(
  effort_census = creel$effort |> filter(tie_in_indicator == 1),
  effort_index = creel$effort |> filter(tie_in_indicator == 0),
  interview = creel$interview
)

```


## dates ** user inputs for fishery closures / "river out" days ** 

First, build an "expanded dates lattice" to which any/all observations are attached, ensuring complete cases.

Start with presumption that all dates are open, then join per-section closures by date.

```{r d_days, include = FALSE}
#creel$event |> distinct(event_date)
d_days <- tibble(
  event_date = seq(
    as.Date(params$date_start, "%Y-%m-%d"),
    as.Date(params$date_end, "%Y-%m-%d"),
    by = "day")
) |> 
  mutate(
    Day = weekdays(event_date),
    DayType = if_else(Day == "Saturday" | Day == "Sunday" | Day %in% dates_holidays_2015_2030, "Weekend", "Weekday"),
    DayType_num = if_else(str_detect(DayType, "end"),1,0),
    DayL = suncalc::getSunlightTimes(
      date = event_date,
      tz = "America/Los_Angeles",
      #need to add flexibility for other rivers/multiple lines in River.Locations lut
      lat = lut$river_loc$Lat,
      lon = lut$river_loc$Long,
      keep=c("dawn", "dusk")
    ) |> 
      mutate(DayL = as.numeric(dusk - dawn)) |>
      pluck("DayL"),
    # Week = as.numeric(format(event_date, "%V")),
    Week = lubridate::isoweek(event_date), #week defined as Monday to SUnday
    Month = as.numeric(format(event_date, "%m")),
    ModelPeriod = params$model_period) |> 
  rowid_to_column(var = "day_index") |> 
  #make open section cols (only those actually used, not all in LU)
  left_join(
    expand_grid(
      event_date = seq(
        as.Date(params$date_start, "%Y-%m-%d"),
        as.Date(params$date_end, "%Y-%m-%d"),
        by = "day")
      ,
      s = paste0("open_section_", sort(unique(point_estimate_data_prelim$effort_index$section)))
    ) |> 
      mutate(closure_code = TRUE) |> 
      pivot_wider(names_from = s, values_from = closure_code)
    ,
    by = "event_date")

# #now add closures if any...

d_days <- rows_upsert(d_days,
    bind_rows(
      # 2022 Upper Skagit Chinook 
      # tibble(section = "1", closure_begin = "2021-05-31", closure_end = "2021-05-31")
      # , 
      
      # 2021 Skagit winter steelhead closure dates
      # tibble(section = "1,2", closure_begin = "2021-02-03", closure_end = "2021-02-05"), 
      # tibble(section = "1,2", closure_begin = "2021-02-10", closure_end = "2021-02-12"), 
      # tibble(section = "1,2", closure_begin = "2021-02-17", closure_end = "2021-02-19"), 
      # tibble(section = "1,2", closure_begin = "2021-02-24", closure_end = "2021-02-26"), 
      # tibble(section = "1,2", closure_begin = "2021-03-03", closure_end = "2021-03-05"), 
      # tibble(section = "1,2", closure_begin = "2021-03-10", closure_end = "2021-03-12"), 
      # tibble(section = "1,2", closure_begin = "2021-03-17", closure_end = "2021-03-19"), 
      # tibble(section = "1,2", closure_begin = "2021-03-24", closure_end = "2021-03-26"), 
      # tibble(section = "1,2", closure_begin = "2021-03-31", closure_end = "2021-04-02"), 
      # tibble(section = "1,2", closure_begin = "2021-04-07", closure_end = "2021-04-09") 
      # tibble(section = "1", closure_begin = "2016-04-30", closure_end = "2016-04-30") # placeholder closure date; need to supply at least one
      
      # kalama 
      tibble(section = "1,2,3", closure_begin = "2016-04-30", closure_end = "2016-04-30") # placeholder closure date; need to supply at least onedate here because we use open / closed dates to determine times_strata estimation periods (field n_days further down in code)
      # ,
      # tibble(section = "2,3", closure_begin = "2021-08-19", closure_end = "2021-08-31"),
      # tibble(section = "2", closure_begin = "2021-09-01", closure_end = "2021-09-01"), # treaty fishery closure
      # tibble(section = "2", closure_begin = "2021-09-07", closure_end = "2021-09-09"), # treaty fishery closure
      # tibble(section = "2", closure_begin = "2021-09-14", closure_end = "2021-09-16"), # treaty fishery closure
      # tibble(section = "1,2,3", closure_begin = "2021-09-18", closure_end = "2021-09-18"), # river out due to flows
      # tibble(section = "2", closure_begin = "2021-10-05", closure_end = "2021-10-06"), # treaty fishery closure
      # tibble(section = "2", closure_begin = "2021-10-12", closure_end = "2021-10-13"), # treaty fishery closure
      # tibble(section = "2", closure_begin = "2021-10-19", closure_end = "2021-10-20"), # treaty fishery closure
      # tibble(section = "1,2,3", closure_begin = "2021-11-13", closure_end = "2021-11-14"), # river out due to flows
      # tibble(section = "1,2,3", closure_begin = "2021-11-17", closure_end = "2021-11-18") # river out due to flows
      
      # tibble(section = "3,4,5", closure_begin = "2021-08-19", closure_end = "2021-08-31"),
      # tibble(section = "3", closure_begin = "2021-09-01", closure_end = "2021-09-01"), # treaty fishery closure
      # tibble(section = "3", closure_begin = "2021-09-07", closure_end = "2021-09-09"), # treaty fishery closure
      # tibble(section = "3", closure_begin = "2021-09-14", closure_end = "2021-09-16"), # treaty fishery closure
      # tibble(section = "1,2,3,4,5", closure_begin = "2021-09-18", closure_end = "2021-09-18"), # river out due to flows
      # tibble(section = "3", closure_begin = "2021-10-05", closure_end = "2021-10-06"), # treaty fishery closure
      # tibble(section = "3", closure_begin = "2021-10-12", closure_end = "2021-10-13"), # treaty fishery closure
      # tibble(section = "3", closure_begin = "2021-10-19", closure_end = "2021-10-20"), # treaty fishery closure
      # tibble(section = "1,2,3,4,5", closure_begin = "2021-11-13", closure_end = "2021-11-14"), # river out due to flows
      # tibble(section = "1,2,3,4,5", closure_begin = "2021-11-17", closure_end = "2021-11-18") # river out due to flows
    ) |>
      rowwise() |>
      mutate(closure_date = paste(seq.Date(as.Date(closure_begin), as.Date(closure_end), by = "day"), collapse = ",")) |>
      separate_rows(closure_date, sep = ",") |>
      select(event_date = closure_date, section) |>
      mutate(
        event_date = as.Date(event_date),
        closure_code = FALSE # TB - The 1e-06 is needed to keep the model from crashing ?log-normal parameters cant be 0
      ) |>
      separate_rows(section, sep = ",") |>
      pivot_wider(names_from = section, names_prefix = "open_section_", values_from = closure_code) |>
      mutate(across(starts_with("open_section_"), ~replace_na(., TRUE)))
    ,
    by ="event_date"
  )

```

## evaluate completeness of index counts / interviews 

```{r count and interview evaluation, include = FALSE}

data_eval <- list()

# Visualize raw counts of vehicles and trailers during monitoring period

# extract the index count types of interest from the YAML and use these to filter the data
# index_count_types <- c(str_split(string = params$index_count_types, pattern = ",")[[1]])
# 
# data_eval$index_counts <-  creel$effort |>
#   filter(tie_in_indicator == 0, count_type %in% index_count_types) |>
#   group_by(section, event_date, count_sequence, count_type) |>
#   summarise(count_quantity = sum(count_quantity), .groups = "drop") |>
#   pivot_wider(names_from = count_type, values_from = count_quantity) |>
#   pivot_longer(names_to = "count_type", values_to = "count_quantity", cols = starts_with(c("B"))) |>
#   arrange(event_date, section)
# 
# view(data_eval$index_counts)
# 
# data_eval$index_counts |>
# ggplot(aes(x = event_date, y = count_quantity)) +
#   geom_point(aes(fill=count_type), colour="black",pch=21, size=2) +
#   scale_fill_manual(values=c("orange", "blue")) +
#   theme_bw() +
#   facet_wrap(.~ section)


# Assess data completeness by looking for deviations from the expected count of distinct effort count locations per section per day. Note - days can look complete if "no count" has been entered for a site, so this is just one view into potential data completeness issues
# data_eval$location_check <- creel$effort |>
#   filter(tie_in_indicator == 0) |>
#   select(event_date, section, location, count_sequence) |>
#   group_by(event_date, section, count_sequence) |>
#   summarise(
#     n_distinct_locations =  n_distinct(location),
#     distinct_locations = paste(unique(location), collapse = ","))
# 
# data_eval$location_check |>  ggplot(aes(x = event_date, y = n_distinct_locations)) +
#   geom_point() +
#   theme_bw() +
#   facet_wrap(.~ section)

# Look at records where "no_count_reason" was used
# data_eval$no_count_records <- creel$effort |>
#   filter(tie_in_indicator == 0, !is.na(no_count_reason)) |>
#   arrange(event_date)

# Evaluate on a daily basis the number of boat and bank interviews obtained
# data_eval$interview <- creel$interview |>
#   mutate(
#     interview_type = if_else(angler_type == "Bank", "Bank", "Boat")
#   ) |>
#   group_by(event_date, section, location, interview_type) |>
#   summarize(
#     n_interviews = n()
#   ) |>
#   pivot_wider(names_from = interview_type, values_from = n_interviews, values_fill = 0) |>
#   pivot_longer(names_to = "interview_type", values_to = "n_interviews", cols = starts_with("B")) |>
#   arrange(event_date, section)

# data_eval$interview |>
# ggplot(aes(x = event_date, y = n_interviews)) +
#   geom_point(aes(fill=interview_type), colour="black",pch=21, size=2) +
#   scale_fill_manual(values=c("orange", "blue")) +
#   theme_bw() +
#   facet_wrap(.~ section)

```

## fishery specific data  ** requries user inputs if post-database data cleaning needed ** 

```{r data clean up, include = FALSE}

# chunk for fishery specific data QA/QC issues

# Skagit issues which may or may not occur elsewhere include missing locations from drive around index effort counts and a location that was supposed to be monitored, but wasn't, for over half of the season (Skagit City Access). Would be good see data issues from other creels to think about what's general vs. project specific. 


# effort count locations with data completeness problems 
# data_eval$remove_location <- bind_rows(
#   tibble(location = "Skagit City Access (SF Skagit, N. end of Fir Island)"),
#   tibble(location = "Rasar State Park"))
# 
# # mistakes in data collection led to instances of incomplete effort counts, this chunk provides the user an option to censor those data by filtering them out of the final dataset 
# # count_id is a paste of event_date, section, count_sequence 
# 
# creel$effort <- creel$effort |>
#    mutate(
#     count_id = paste(event_date, section, count_sequence, sep = "_")
#   )
# 
# # declare individual count_sequences which contain missing counts and could / should be censored from analysis 
# 
# data_eval$remove_count <- bind_rows(
#     tibble(count_id = "2021-08-19_1_1"),
#     tibble(count_id = "2021-08-20_1_3"),
#     tibble(count_id = "2021-09-17_2_4"),
#     tibble(count_id = "2021-09-19_2_2"),
#     tibble(count_id = "2021-09-23_3_1"),
#     tibble(count_id = "2021-09-23_2_1"),
#     tibble(count_id = "2021-09-23_1_3"),
#     tibble(count_id = "2021-09-26_1_3"),
#     tibble(count_id = "2021-09-26_2_2"),
#     tibble(count_id = "2021-10-21_3_3"),
#     tibble(count_id = "2021-10-31_3_1")
#     )
# 
# # n rows in effort data pre filter
# data_eval$counts_pre_filter <- nrow(creel$effort)
# 
# creel$effort <- creel$effort |>
#   filter(!location %in% data_eval$remove_location$location, !count_id %in% data_eval$remove_count$count_id) |>
#   arrange(event_date, section, count_sequence)
# 
# # # n rows in effort data post filter
# data_eval$counts_post_filter <- nrow(creel$effort)
# 
# # number of counts removed from dataset
# print(data_eval$counts_pre_filter - data_eval$counts_post_filter)
# 
# # Placeholder as option to replace NA's with 0's if/when deemed appropriate
# creel$effort <- creel$effort |>
# mutate(count_quantity = if_else(!is.na(no_count_reason), 0, count_quantity))


```



## effort data 

### effort census

Aggregate census (tie in) effort counts, associating to closest-in-time index count. 

```{r point_estimate_data_prelim_effort_census, include = FALSE}
#to the initial effort_census, with all count_sequence == 1,
#add/overwrite the count_sequence val with that from closest temporal match from inline/anonymous paired counts object 

point_estimate_data_prelim$effort_census <- point_estimate_data_prelim$effort_census |> 
  select(event_date, water_body, water_body_desc, location, section, tie_in_indicator, count_type, count_quantity) |> 
  left_join(
    left_join(
      point_estimate_data_prelim$effort_census |> distinct(event_date, section, location, tie_in_indicator, effort_start_time, count_sequence),
      point_estimate_data_prelim$effort_index |> distinct(event_date, section, location, tie_in_indicator, effort_start_time, count_sequence),
      by = c("event_date", "section"),
      suffix = c("_cen", "_ind")
      ) |> 
      group_by(event_date, section, location_cen) |> 
      slice_min(abs(effort_start_time_cen - effort_start_time_ind), n = 1) |> 
      ungroup() |> 
      #count(event_date, section, location_cen, count_sequence_cen, count_sequence_ind)
      distinct(event_date, section, location = location_cen, count_sequence = count_sequence_ind)
    ,
    by = c("event_date", "section", "location")
  ) |> 
  left_join(d_days, by = "event_date") |>
  mutate(
    #angler_type = word(count_type, 1),
    angler_type = case_when(
      word(count_type, 1) %in% c("Bank","bank","Shore") ~ "bank",
      word(count_type, 1) %in% c("Boat","boat") ~ "boat"
    )
  ) |>
  #exclude any count_type strings we didn't whitelist into an angler_type
  #e.g., "Boats" which are not a thing we use because reasons. 
  filter(!is.na(angler_type)) |> 
  group_by(event_date, day_index, section, count_sequence, angler_type) |>
  summarize(count_quantity = sum(count_quantity), .groups = "drop") |>
  mutate(tie_in_indicator = 1) |> 
  arrange(event_date, section, count_sequence) |> 
  drop_na(count_sequence)


## Kalama census count object with location retained

# kalama_census_join <- point_estimate_data_prelim$effort_census |> 
#   select(event_date, water_body, water_body_desc, location, section, tie_in_indicator, count_type, count_quantity) |> 
#   left_join(
#     left_join(
#       point_estimate_data_prelim$effort_census |> distinct(event_date, section, location, tie_in_indicator, effort_start_time, count_sequence),
#       point_estimate_data_prelim$effort_index |> distinct(event_date, section, location, tie_in_indicator, effort_start_time, count_sequence),
#       by = c("event_date", "section"),
#       suffix = c("_cen", "_ind")
#       ) |> 
#       group_by(event_date, section, location_cen) |> 
#       slice_min(abs(effort_start_time_cen - effort_start_time_ind), n = 1) |> 
#       ungroup() |> 
#       #count(event_date, section, location_cen, count_sequence_cen, count_sequence_ind)
#       distinct(event_date, section, location = location_cen, count_sequence = count_sequence_ind)
#     ,
#     by = c("event_date", "section", "location")
#   ) |> 
#   left_join(d_days, by = "event_date") |>
#   mutate(
#     #angler_type = word(count_type, 1),
#     angler_type = case_when(
#       word(count_type, 1) %in% c("Bank","bank","Shore") ~ "bank",
#       word(count_type, 1) %in% c("Boat","boat") ~ "boat"
#     )
#   ) |>
#   #exclude any count_type strings we didn't whitelist into an angler_type
#   #e.g., "Boats" which are not a thing we use because reasons. 
#   filter(!is.na(angler_type)) |> 
#   group_by(event_date, day_index, location, section, count_sequence, angler_type) |>
#   summarize(count_quantity = sum(count_quantity), .groups = "drop") |>
#   mutate(tie_in_indicator = 1) |> 
#   arrange(event_date, section, count_sequence) |> 
#   drop_na(count_sequence)




```

### effort index

Aggregate index counts of vehicles, trailers, anglers, and boats.

```{r point_estimate_data_prelim_effort_index, include = FALSE}

point_estimate_data_prelim$effort_index <- point_estimate_data_prelim$effort_index |> 
  filter(
    is.na(no_count_reason),
    !is.na(count_type),
    count_type %in% index_count_types # filter to analysis specific index count types 
    ) |>  
  select(event_date, water_body, water_body_desc, location, section, tie_in_indicator, count_sequence, count_type, count_quantity) |>
  left_join(d_days, by = "event_date") |> 
  group_by(section, event_date, day_index, DayL, Week, Month, count_sequence, count_type) |> 
  summarise(count_quantity = sum(count_quantity), .groups = "drop") |> 
  arrange(event_date, section, count_sequence) 

## Kalama index join with location retained
# kalama_index_join <- point_estimate_data_prelim$effort_index |> 
#   filter(
#     is.na(no_count_reason),
#     !is.na(count_type),
#     count_type %in% index_count_types # filter to analysis specific index count types 
#     ) |>  
#   select(event_date, water_body, water_body_desc, location, section, tie_in_indicator, count_sequence, count_type, count_quantity) |>
#   left_join(d_days, by = "event_date") |> 
#   group_by(location, section, event_date, day_index, DayL, Week, Month, count_sequence, count_type) |> 
#   summarise(count_quantity = sum(count_quantity), .groups = "drop") |> 
#   arrange(event_date, section, count_sequence) 


```


### effort totals per count_sequence and mean daily values of vehicle and trailer counts 
```{r point_estimate_data_prelim$mean_daily_effort_index_counts, include = FALSE}
# average index effort count by day, angler_type, and section, which is the mean value of the sum total of effort counts conducted during n number of count_sequence within a day 

# sum total of vehicle trailer counts per date, section, sampling event (count_sequence), and count_type (vehicles, trailers, etc.)
point_estimate_data_prelim$Daily_effort_per_count_index_counts <- point_estimate_data_prelim$effort_index |> 
  group_by(event_date, section, count_sequence, count_type) |> 
  summarize(
    sum_index_count = sum(count_quantity)
  ) 

# Mean count across multiple sampling events within a day 

point_estimate_data_prelim$mean_daily_effort_index_counts <- point_estimate_data_prelim$Daily_effort_per_count_index_counts|> 
  group_by(event_date, section, count_type) |> 
  summarise(
    mean_index_count = mean(sum_index_count)
  ) |> 
  mutate(
    angler_type =
    case_when(
      count_type == "Boat Anglers" ~ "boat",
      count_type == "Bank Anglers" ~ "bank",
      count_type == "Trailers Only" ~ "boat",
      count_type == "Vehicle Only" ~ "total"
    )
  )

```


## interview data 

```{r point_estimate_data_prelim_interview, include = FALSE}

# derive needed fields from interview data and join fish catch data 

point_estimate_data_prelim$interview <- point_estimate_data_prelim$interview |> 
  left_join(d_days, by = "event_date") |> #summary()
  mutate(
    across(c(vehicle_count, trailer_count), ~replace_na(., 0)),
    trip_status = replace_na(trip_status, "Unknown"),
    angler_type_check = all(is.na(angler_type)), # check to see if angler_type field is used for a given dataset 
    angler_type = tolower(angler_type),
    angler_type = ifelse(angler_type_check == TRUE, boat_used, angler_type), # if it's TRUE that all values are NA in angler_type field, then use the boat_used field instead. Could use DA's brain to ensure this is robust, used ifelse( ) so that the FALSE could be different data class
    angler_type =
      case_when( #case_when to convert all possible entries into the angler_type field to angler_types
      # is.na(angler_type) ~ "bank",
      angler_type == "boat" ~ "boat",
      angler_type == "bank" ~ "bank",
      angler_type == "Unk" ~ "bank", # Kalama placeholder 
      is.na(angler_type) ~ "bank", # Kalama placeholder 
      angler_type == "No" ~ "bank",
      angler_type == "Yes" ~ "boat"
      ),
    angler_type_ind = as.integer(factor(angler_type)),
    # fishing_end_time = if_else(is.na(fishing_end_time), interview_time, fishing_end_time),
    fishing_end_time = if_else(trip_status == "Incomplete", interview_time, fishing_end_time), # fishing_end_time's were wrong in Kalama dataset, using this but should verify with newer datasets 
    angler_hours = round(as.numeric(fishing_end_time - fishing_start_time) / 3600, 5),
    angler_hours_total = angler_count * angler_hours,
    time_strata = if_else(ModelPeriod == "Month", Month, Week)
  ) |> 
  filter(angler_hours_total >= 0.5)

```



# daily estimates of effort 

Here calculating estimates of daily effort (angler-hours) from vehicle / trailer counts (index counts) and angler group data (interviews), following methods used in North Puget Sound creels. 

An exception to this is the Cascade River, where only vehicles get counted, since there's no boat angling / access on the 1 mile long fishery. 

## join daily angler data from interviews to daily summaries of index counts 
```{r point_estimate_data_prelim$daily_effort_estimates, include = FALSE}

# summarize angler data from interviews

# if index counts are vehicles and trailers, calculate angler per vehicle ("total" represents all anglers in the fishery) and angler per trailer ("boat") values

if(str_detect(index_count_types, c("Vehicle Only", "Trailers Only"))){ 

point_estimate_data_prelim$interviews_total_effort <- point_estimate_data_prelim$interview |>
  group_by(section, event_date) |> 
  summarize(
    daily_sum_angler = sum(angler_count),
    daily_sum_index_count_from_interview = sum(vehicle_count),
    angler_hours_total = sum(angler_hours_total),
    anglers_per_index_count_from_interview = daily_sum_angler / daily_sum_index_count_from_interview) |>
  mutate(
    angler_type = "total"
  )

point_estimate_data_prelim$interviews_boat_effort <- point_estimate_data_prelim$interview |>
  filter(angler_type == "boat") |>
  group_by(section, event_date) |>
  summarize(
    daily_sum_angler = sum(angler_count),
    daily_sum_index_count_from_interview = sum(vehicle_count),
    angler_hours_total = sum(angler_hours_total),
    anglers_per_index_count_from_interview = daily_sum_angler / daily_sum_index_count_from_interview) |>
  mutate(
    angler_type = "boat")

point_estimate_data_prelim$angler_data_from_interviews <- point_estimate_data_prelim$interviews_total_effort |> 
  bind_rows(point_estimate_data_prelim$interviews_boat_effort) 
        
        }

# Day length expansion of angler hours  

if(str_detect(index_count_types, c("Bank Anglers", "Boat Anglers"))) {

point_estimate_data_prelim$daily_effort_estimates <- point_estimate_data_prelim$mean_daily_effort_index_counts |>
  left_join(d_days, by = "event_date") |> 
    mutate(
    mean_daily_effort = DayL * mean_index_count) |>
  arrange(event_date, section)

      }else{
      if(str_detect(index_count_types, c("Vehicle Only", "Trailers Only"))){

point_estimate_data_prelim$daily_effort_estimates <- point_estimate_data_prelim$angler_data_from_interviews  |>
  left_join(point_estimate_data_prelim$mean_daily_effort_index_counts, by = c("event_date", "section", "angler_type")) |>
  left_join(d_days, by = "event_date") |> 
    mutate(
    mean_daily_effort = DayL * anglers_per_index_count_from_interview * mean_index_count) |>
  arrange(event_date, section) |> 
filter(!is.na(mean_index_count)) #circle back on this 

      }
}

# for data with trailer / vehicle counts, check to see if derived bank effort from total minus boat results in any negative values 

# c1 <- point_estimate_data_prelim$daily_effort_estimates |>
#   ungroup() |>
#   filter(angler_type == "boat") |>
#   select(event_date, section, boat_effort = mean_daily_effort)
# 
# c2 <- point_estimate_data_prelim$daily_effort_estimates |>
#   ungroup() |>
#   filter(angler_type == "total") |>
#   select(event_date, section, total_effort = mean_daily_effort)
# 
# c3 <- c2 |>
#   left_join(c1, by = c("event_date", "section")) |>
#   mutate(boat_effort = replace_na(boat_effort, 0)) |>
#   mutate(
#     bank_derive = total_effort - boat_effort
#   )

```


# evaluate days with measured effort by no interview

```{r stan_data_prelim_interview_daily_totals, include = FALSE}

# total hours from angler interviews and total catch sample a per day/angler type/section to compare with effort

# point_estimate_data_prelim$interview_daily_totals <- point_estimate_data_prelim$interview |>
#   group_by(event_date, day_index, time_strata, section, angler_type) |>
#   summarise(
#     angler_hours_total_dailysum = sum(angler_hours_total),
#     catch_dailysum = sum(fish_count), .groups = "drop"
#   ) |> 
#   select(event_date, section, angler_type, angler_hours_total_dailysum, catch_dailysum)
# 
# point_estimate_data_prelim$effort_no_interview <- point_estimate_data_prelim$mean_daily_effort |> 
#   left_join(point_estimate_data_prelim$angler_data_from_interviews, by = c("event_date", "section", "angler_type")) |> 
#   filter(is.na(angler_hours_total)) |> 
#   arrange(section, event_date)
# 
# print(point_estimate_data_prelim$effort_no_interview)

# these are days we could / should? "assume" CPUE / hpue using something standardized, consistent, and reproducible

```


## join census count data to index count data and apply bias term ratio to unexpanded index counts ** user inputs on section and angler_types to right join unsampled days to sampled days ** 

```{r point_estimate_data_prelim$effort_interviews_final #1, include = FALSE}

# calculate mean vehicle and trailer counts and corresponding anglers per vehicle / anglers per trailer by day, section, and count sequence to match temporal scale of census counts 

# days_join object to add all days in monitoring period into table for calculation of counts of sampled days (n_days) and total days in monitoring period (N_days) with a right_join()
# realized needed rows duplicated across angler_type and section for join later in script, so this is the current workaround 

# if index_count types are bank angler, boat anglers, then at_1 = bank, at_2 = boat (no total angler category)
# if index_count type are vehicle only, trailers only then at_1 = bank, at_2 = boat, at_3 = total
# need to change this so it dynamically adjusts to index_count_types

# Kalama exmaple
point_estimate_data_prelim$days_join <- d_days |>
  select(event_date, DayType, Month, Week, ModelPeriod, open_section_1) |>
  mutate(
    time_strata = if_else(ModelPeriod == "Month", Month, Week),
    at_1 = "bank", at_2 = "boat"
  ) |>
  pivot_longer(cols = c(at_1,at_2), values_to = "angler_type") |>
  select(-name) |>
  mutate(s1 = 1, s2 = 2, s3 = 3
  ) |>
  pivot_longer(cols = c(s1,s2,s3), values_to = "section") |>
  select(-name)

# Skagit winter steelhead 2021
# point_estimate_data_prelim$days_join <- d_days |> 
#   select(event_date, DayType, Month, Week, ModelPeriod, open_section_1) |> 
#   mutate(
#     time_strata = if_else(ModelPeriod == "Month", Month, Week),
#     at_1 = "bank", at_2 = "boat", at_3 = "total"
#   ) |> 
#   pivot_longer(cols = c(at_1,at_2,at_3), values_to = "angler_type") |> 
#   select(-name) |> 
#   mutate(s1 = 1, s2 = 2
#   ) |> 
#   pivot_longer(cols = c(s1,s2), values_to = "section") |> 
#   select(-name)

# Skagit fall coho 2021
# point_estimate_data_prelim$days_join <- d_days |>
#   select(event_date, DayType, Month, Week, ModelPeriod, open_section_1) |>
#   mutate(
#     time_strata = if_else(ModelPeriod == "Month", Month, Week),
#     at_1 = "bank", at_2 = "boat", at_3 = "total"
#   ) |>
#   pivot_longer(cols = c(at_1,at_2,at_3), values_to = "angler_type") |>
#   select(-name) |>
#   mutate(s1 = 1
#   ) |>
#   pivot_longer(cols = c(s1), values_to = "section") |>
#   select(-name)


if(str_detect(index_count_types, c("Bank Anglers", "Boat Anglers"))) {

point_estimate_data_prelim$Daily_effort_per_count_groups <- point_estimate_data_prelim$Daily_effort_per_count_index_counts |>
  left_join(d_days, by = "event_date") |> 
  mutate(
    angler_type = if_else(count_type == "Boat Anglers", "boat", "bank")
  ) |> 
  select(-count_type) 
  
# index count derived estimates of angler counts

point_estimate_data_prelim$effort_counts_for_census_join <- point_estimate_data_prelim$Daily_effort_per_count_groups |> 
  rename(index_count =  sum_index_count) |> 
  select(event_date, section, DayL, count_sequence, angler_type, index_count)
  
# left join effort counts to census counts 

# calculate boat anglers counted per census count
point_estimate_data_prelim$census_counts_both_types <- point_estimate_data_prelim$effort_census |>
  group_by(event_date, section, angler_type, day_index, count_sequence) |> 
  summarize(
    count_quantity = sum(count_quantity)
  )

# calculate bias term ratios (ratio of census counts to angler count estimates from index counts)
# as-is using data grouped by section; needt to add functionality for census data collected at finer spatial / temporal scales
point_estimate_data_prelim$census_counts_all <- point_estimate_data_prelim$census_counts_both_types |> 
  select(
    event_date, section, count_sequence, 
    census_count = count_quantity, angler_type
  ) |> 
  left_join(point_estimate_data_prelim$effort_counts_for_census_join, by = c("event_date", "section", "count_sequence","angler_type")) |>
  mutate(
  TI_expan = census_count / index_count,
  TI_expan = ifelse(!is.infinite(TI_expan), TI_expan, NA)) |> 
  filter(!is.nan(TI_expan)) |> 
  group_by(section, angler_type) |> 
  summarise(
    sum_census_anglers = sum(census_count),
    sum_index_anglers = sum(index_count),
    TI_expan_mean = mean(TI_expan, na.rm=TRUE),
    TI_expan_weighted = sum(census_count) / sum(index_count)) |> 
  mutate(
    TI_expan_weighted = if_else(TI_expan_weighted == 0, 1, TI_expan_weighted),
    TI_expan_mean = if_else(TI_expan_mean == 0, 1, TI_expan_mean)
  ) |> 
  left_join(lut$census_expansion, by = c("angler_type", "section")) |>
  rename(TI_expan_indirect = Indirect_TI_Expan) |> 
  mutate(
    census_expansion_method = params$census_expansion,
    TI_expan_mean = replace_na(TI_expan_mean, 1),
    TI_expan_weighted = replace_na(TI_expan_weighted, 1),
    TI_expan_direct = TI_expan_weighted / p_TI) |> 
  mutate(
    TI_expan_final = ifelse(census_expansion_method == "direct", TI_expan_direct, TI_expan_indirect)
  ) 

# join census (tie in) counts to index counts 

# calculate mean daily effort multipled by bias term (tie in ratio) from census counts

point_estimate_data_prelim$effort_interviews_bank_boat <- point_estimate_data_prelim$daily_effort_estimates |>
  ungroup() |> 
  left_join(point_estimate_data_prelim$census_counts_all, by = c("section", "angler_type")) |>
  mutate(
    TI_expan_final = if_else(is.na(TI_expan_final), 1, TI_expan_final) 
  ) |> 
  mutate(
    mean_daily_TI_expan = mean_daily_effort * TI_expan_final
  ) 


point_estimate_data_prelim$effort_interviews_final <- point_estimate_data_prelim$effort_interviews_bank_boat |>
  select(section, event_date, angler_type, mean_daily_TI_expan) |> 
  right_join(point_estimate_data_prelim$days_join, by = c("section", "event_date", "angler_type")) |>
  mutate(
    creeled = if_else(!is.na(mean_daily_TI_expan) & open_section_1 == TRUE, "Y", "N")
  ) |> 
  arrange(event_date)
  
  
      }else{
      if(str_detect(index_count_types, c("Vehicle Only", "Trailers Only"))){
        
point_estimate_data_prelim$Daily_effort_per_count_groups <- point_estimate_data_prelim$Daily_effort_per_count_index_counts |>
  mutate(
    angler_type = if_else(count_type == "Trailers Only", "boat", "total")
  ) |> 
  select(-count_type) 
  
# index count derived estimates of angler counts

point_estimate_data_prelim$effort_counts_for_census_join <- point_estimate_data_prelim$angler_data_from_interviews |> 
  left_join(point_estimate_data_prelim$Daily_effort_per_count_groups, by = c("event_date", "section", "angler_type")) |> 
  mutate(
    index_count = anglers_per_index_count_from_interview * sum_index_count
  ) |> 
  ungroup() |> 
  select(event_date, section, count_sequence, angler_type, index_count)
  
# left join effort counts to census counts 

# calculate total anglers counted per census count (boat + bank anglers) 

point_estimate_data_prelim$census_counts_total <- point_estimate_data_prelim$effort_census |> 
  group_by(event_date, section, count_sequence) |> 
  summarize(
    count_quantity = sum(count_quantity)
  ) |>
  mutate(
    angler_type = "total"
  )

# calculate boat anglers counted per census count

point_estimate_data_prelim$census_counts_boat <- point_estimate_data_prelim$effort_census |>
  filter(angler_type == "boat") |>
  group_by(event_date, section, count_sequence) |>
  summarize(
    count_quantity = sum(count_quantity)
  ) |>
  mutate(
    angler_type = "boat"
  )
    
# Bind these together so we have categories for 1) total hours to match with anglers per vehicle and 2) boat hours to match with anglers per trailer 
# calculate bias term ratios (ratio of census counts to angler count estimates from index counts)
# as-is using data grouped by section; needt to add functionality for census data collected at finer spatial / temporal scales
point_estimate_data_prelim$census_counts_all <- point_estimate_data_prelim$census_counts_total |> 
  bind_rows(point_estimate_data_prelim$census_counts_boat) |> 
   select(
    event_date, section, count_sequence, 
    census_count = count_quantity, angler_type
  ) |> 
  left_join(point_estimate_data_prelim$effort_counts_for_census_join, by = c("event_date", "section", "count_sequence","angler_type")) |>
  mutate(
  TI_expan = census_count / index_count,
  TI_expan = ifelse(!is.infinite(TI_expan), TI_expan, NA)) |> 
  filter(!is.nan(TI_expan)) |> 
  group_by(section, angler_type) |> 
  summarise(
    sum_census_anglers = sum(census_count),
    sum_index_anglers = sum(index_count),
    TI_expan_mean = mean(TI_expan, na.rm=TRUE),
    TI_expan_weighted = sum(census_count) / sum(index_count)) |> 
  mutate(
    TI_expan_weighted = if_else(TI_expan_weighted == 0, 1, TI_expan_weighted),
    TI_expan_mean = if_else(TI_expan_mean == 0, 1, TI_expan_mean)
  ) |> 
  left_join(lut$census_expansion, by = c("angler_type", "section")) |>
  rename(TI_expan_indirect = Indirect_TI_Expan) |> 
  mutate(
    census_expansion_method = params$census_expansion,
    TI_expan_mean = replace_na(TI_expan_mean, 1),
    TI_expan_weighted = replace_na(TI_expan_weighted, 1),
    TI_expan_direct = TI_expan_weighted / p_TI) |> 
  mutate(
    TI_expan_final = ifelse(census_expansion_method == "direct", TI_expan_direct, TI_expan_indirect)
  ) 


# join census (tie in) counts to index counts 

# calculate mean daily effort multipled by bias term (tie in ratio) from census counts
point_estimate_data_prelim$effort_interviews_totalandboat <- point_estimate_data_prelim$daily_effort_estimates |>
  ungroup() |> 
  left_join(point_estimate_data_prelim$census_counts_all, by = c("section", "angler_type")) |> 
  # select(-c(time_strata)) |> 
  mutate(
    mean_daily_TI_expan = mean_daily_effort * TI_expan_final
  ) 

# subtract boat effort from the total effort to estimate bank effort. 

point_estimate_data_prelim$effort_interviews_bank_1 <- point_estimate_data_prelim$effort_interviews_totalandboat |>
  select(section, event_date, angler_type, mean_daily_TI_expan) |>
  pivot_wider(names_from = angler_type, values_from = mean_daily_TI_expan, values_fill = 0) |>
  mutate(
    mean_daily_TI_expan_bank = total - boat
  )
  
# joining estimated bank effort to initial table with total and boat hours and then using distinct call to filter to only the newly derived bank hours while retaining the other table data needed to bind this data back to the initial dataset 
point_estimate_data_prelim$effort_interviews_bank_2 <- point_estimate_data_prelim$effort_interviews_totalandboat |>
  left_join(point_estimate_data_prelim$effort_interviews_bank_1, by = c("section", "event_date")) |>
  distinct(section, event_date, mean_daily_TI_expan_bank, .keep_all = TRUE) |>
  mutate(
    angler_type = "bank"
  ) |>
  select(-c(mean_daily_TI_expan, total, boat)) |>
  rename(
    mean_daily_TI_expan = mean_daily_TI_expan_bank
  )

# Binding the bank hours back to the initial table
point_estimate_data_prelim$effort_interviews_final <- point_estimate_data_prelim$effort_interviews_totalandboat |> 
  bind_rows(point_estimate_data_prelim$effort_interviews_bank_2) |> 
  ungroup() |> 
  # right_join(days_join, by = c("section", "event_date", "angler_type")) |> 
  right_join(point_estimate_data_prelim$days_join) |> 
  mutate(
    creeled = if_else(!is.na(mean_daily_TI_expan), "Y", "N")
  ) |> 
  arrange(event_date)
      }
      }

```

## calculate number of sampled and unsampled days per day type and time period strata, add to daily effort-interview dataset  ** requires user input **

```{r point_estimate_data_prelim$effort_interviews_final #2, include = FALSE}

# total number days within monitoring period that we want to generate estimates for. Excludes specified closures in dates object

point_estimate_data_prelim$total_days <- d_days |>
  pivot_longer(cols = starts_with("open_section"), names_to = "closure_section", values_to = "closure_value") |>
  mutate(section = as.numeric(gsub("^.*_", "", closure_section)),
         time_strata = if_else(ModelPeriod == "Month", Month, Week)) |>
  filter(closure_value == TRUE) |> 
  count(section, DayType, time_strata, name = "N_days")

# number of days sampled within monitoring period

point_estimate_data_prelim$sampled_days <- point_estimate_data_prelim$effort_interviews_final |>
  filter(!is.na(mean_daily_TI_expan) & creeled == "Y") |> 
  count(section, DayType, time_strata, angler_type, name = "n_days")

# total number days within monitoring period that we want to generate estimates for. Excludes specified closures in dates object

point_estimate_data_prelim$total_days_total <- d_days |>
  pivot_longer(cols = starts_with("open_section"), names_to = "closure_section", values_to = "closure_value") |>
  mutate(section = as.numeric(gsub("^.*_", "", closure_section)),
         time_strata = if_else(ModelPeriod == "Month", Month, Week)) |> 
  filter(closure_value == TRUE) |> 
  count(section, DayType, name = "N_days_total")

# number of days sampled within monitoring period

point_estimate_data_prelim$sampled_days_total <- point_estimate_data_prelim$effort_interviews_final |>
  filter(!is.na(mean_daily_TI_expan) & creeled == "Y") |> 
count(section, DayType, angler_type, name = "n_days_total")


# add these objects to the table with index count data

point_estimate_data_prelim$effort_interviews_final <- point_estimate_data_prelim$effort_interviews_final |> 
  left_join(point_estimate_data_prelim$total_days, by = c("section", "DayType", "time_strata")) |> 
  left_join(point_estimate_data_prelim$sampled_days, by = c("section", "DayType", "time_strata", "angler_type")) |>
  left_join(point_estimate_data_prelim$total_days_total, by = c("section", "DayType")) |> 
  left_join(point_estimate_data_prelim$sampled_days_total, by = c("section", "DayType", "angler_type")) |> 
  filter(!is.na(mean_daily_TI_expan)) 

# now, need daily CPUE on sampled days, and need to identify days where effort was sampled but no corresponding interviewed was obtained to estimate CPUE

```


# daily catch per unit effort (CPUE) from interviews 
```{r daily CPUE from interviews, include = FALSE}

# Calculating CPUE for the different angler_types

point_estimate_data_prelim$catch_data <- creel$catch |> 
    group_by(interview_id, catch_group) |> 
    summarise(fish_count = sum(fish_count), .groups = "drop") |> 
  mutate(across(fish_count, ~replace_na(., 0)),
        catch_group = replace_na(catch_group, "no_catch"))

# obtain unique catch_groups present in intermediate interview object

point_estimate_data_prelim$species_present <- point_estimate_data_prelim$catch_data |>
  distinct(catch_group)

# # split off fish count data to pivot out on catch groups to extend catch group information to each interview, essentially inferring 0's in interviews when catch for a group is not reported

point_estimate_data_prelim$catch_data_pivot_wide <- point_estimate_data_prelim$catch_data |>
  select(interview_id, catch_group, fish_count) |>
  pivot_wider(names_from = catch_group, values_from = fish_count, values_fill = 0) |> # pivot_wide to infer 0's
  select(interview_id, contains(point_estimate_data_prelim$species_present$catch_group)) |>
  group_by(interview_id) |>
  summarise(across(contains(point_estimate_data_prelim$species_present$catch_group), sum))

# rejoin catch data to interview data and pivot long on counts per catch group

point_estimate_data_prelim$interview_and_catch <- point_estimate_data_prelim$interview |>
  # select(-c(catch_group, fish_count)) |>
  filter(!is.na(angler_hours)) |>
  distinct(interview_id, .keep_all = TRUE) |>
  left_join(point_estimate_data_prelim$catch_data_pivot_wide, by = "interview_id") |>
  pivot_longer(cols = contains(point_estimate_data_prelim$species_present$catch_group), names_to = "catch_group", values_to = "fish_count") |>
  mutate(fish_count = replace_na(fish_count, 0)) 
# |> 
  # filter(catch_group == "Steelhead_Adult_UM_Released")

# check fish_count and angler hours to see if anything is lost during pivot_wide, pivot_long
# point_estimate_data_prelim$interview_and_catch |>
#   filter(catch_group == "Steelhead_Adult_UM_Released") |>
#   summarize(fish_count = sum(fish_count),
#             hours_count = sum(angler_hours_total))


# mean angler hours by section, time_strata, angler_type to use in case there is non-zero catch, but no angler hours to use to calculate CPUE (should be a rare situation)

point_estimate_data_prelim$angler_hours_summary <- point_estimate_data_prelim$interview_and_catch |>
  group_by(section, time_strata, DayType, angler_type, catch_group) |>
  summarise(
    mean_strata_angler_hours = mean(angler_hours_total)
  ) |>
  ungroup() |>
  complete(section, time_strata, DayType, angler_type, catch_group,
           fill = list(mean_strata_angler_hours = 0))

# CPUE for bank and boat effort 

point_estimate_data_prelim$interview_daily_totals <- point_estimate_data_prelim$interview_and_catch |>
  left_join(point_estimate_data_prelim$angler_hours_summary) |> 
   mutate(
    cpue_interview = if_else(angler_hours_total > 0, fish_count / angler_hours_total, fish_count / mean_strata_angler_hours)
  ) |>
  # filter(catch_group == "Steelhead_Adult_UM_Released") |> 
  group_by(section, DayType, event_date, day_index, time_strata, catch_group, angler_type) |>
  summarise(
    n_groups = n(),
    angler_hours_total_dailysum = sum(angler_hours_total),
    catch_dailysum = sum(fish_count), .groups = "drop",
    cpue_mor = mean(cpue_interview)  # mean of ratios CPUE estimator
  ) |>
  mutate(
    cpue_rom = catch_dailysum / angler_hours_total_dailysum # ratio of the means CPUE estimator
  )


# bind together total, boat, and bank tables
# point_estimate_data_prelim$interview_daily_totals <- point_estimate_data_prelim$interview_daily_totals_angler_type
   # |>
#   bind_rows(point_estimate_data_prelim$interview_daily_totals_total)



#  add implicit missing values of CPUE to dataset using tidyr complete() function

point_estimate_data_prelim$interview_daily_totals_complete <- point_estimate_data_prelim$interview_daily_totals |>
  ungroup() |>
  select(section, event_date, angler_type, catch_group,
         total_angler_hours = angler_hours_total_dailysum,
         catch_dailysum, cpue_rom, cpue_mor) |>
  mutate(angler_type = replace_na(angler_type, "bank")) |> #kalama steelhead fix
  # filter(catch_group == "Chinook_Adult_UM_Released") |> # placeholder spot to filter catch groups
  # complete(section, event_date, angler_type, fill = list(catch_dailysum = 0)) |>
  complete(section, event_date, angler_type, catch_group, fill = list(catch_dailysum = 0)) |>
  filter(!(is.na(angler_type))) |>
  # mutate(catch_group = "Chinook_Adult_UNK_Released") |>
  # distinct() |>
  left_join(d_days, by = c("event_date")) |> 
  mutate(
  time_strata = if_else(ModelPeriod == "Month", Month, Week))


# calculate mean CPUE values by strata to use on days with known effort but a lack of interviews 
point_estimate_data_prelim$cpue_summary <- point_estimate_data_prelim$interview_daily_totals |>
  group_by(section, time_strata, DayType, angler_type, catch_group) |>
  summarise(
    mean_strata_cpue_mor = mean(cpue_mor), # Fisheries Techniques (Jones and Pollock 2012) suggest MOR for roving survey design
    mean_strata_cpue_rom = mean(cpue_rom)
  ) |>
  ungroup() |>
  complete(section, time_strata, DayType, angler_type, catch_group,
           fill = list(mean_strata_cpue_mor = 0, mean_strata_cpue_rom = 0))

# calculate mean daily effort by strata to use on days with interviews but a lack of quantified effort
point_estimate_data_prelim$effort_summary <- point_estimate_data_prelim$effort_interviews_final |> 
  filter(!is.na(DayType)) |>
  group_by(section, time_strata, DayType, angler_type) |>
  summarise(
    mean_strata_effort = mean(mean_daily_TI_expan)
  ) |>
  ungroup() |>
  complete(section, time_strata, angler_type,
           fill = list(mean_strata_effort = 0))


point_estimate_data_prelim$effort_join <- point_estimate_data_prelim$effort_interviews_final |> 
  filter(!is.na(DayType)) |> 
  select(section, time_strata, event_date, angler_type, mean_daily_TI_expan, n_days, N_days, n_days_total, N_days_total) |> 
left_join(point_estimate_data_prelim$effort_summary) |> 
  mutate(
    mean_daily_TI_expan = if_else(mean_daily_TI_expan == 0, mean_strata_effort, mean_daily_TI_expan)
  )


# join mean_strata_cpue values to table with daily cpue values so we can use the mean strata cpue values on days with known effort but no corresponding interview
# extract the columns we need from the interview and catch daily total table to calculate daily CPUE

point_estimate_data_prelim$daily_CPUE <- point_estimate_data_prelim$interview_daily_totals_complete |> 
  left_join(point_estimate_data_prelim$cpue_summary, by = c("section", "angler_type", "time_strata", "DayType", "catch_group"))|> 
  select(section, time_strata, event_date, DayType, angler_type, catch_group, catch_dailysum, total_angler_hours, cpue_rom, cpue_mor, mean_strata_cpue_rom, mean_strata_cpue_mor)

```



# daily catch (effort * CPUE) estimates ** user inputs if filtering final catch_groups **
```{r daily catch estimates, include = FALSE}

# empty list for summarized estimates we want to export

estimates <- list()


estimates$daily_effort_cpue_catch <- point_estimate_data_prelim$daily_CPUE  |>
   left_join(point_estimate_data_prelim$effort_join) |>
  mutate(total_angler_hours = replace_na(total_angler_hours, 0)) |> 
    filter(angler_type %in% c("bank","boat")) |>
  mutate(
    catch_estimate = if_else(!is.na(cpue_mor), mean_daily_TI_expan * cpue_mor, mean_daily_TI_expan * mean_strata_cpue_mor)
    # catch_estimate_rom = if_else(!is.na(cpue_rom), mean_daily_TI_expan * cpue_rom, mean_daily_TI_expan * mean_strata_cpue_rom), # option to use ratio of the means CPUE estimator
    # catch_estimate = catch_estimate_mor
    ) |>
  mutate_if(is.numeric, round, 2) |>
  # mutate(
  #   mean_daily_TI_expan = replace_na(mean_daily_TI_expan, 0),
  #   catch_estimate = replace_na(catch_estimate, 0)
  # ) |> 
    # filter(str_detect(catch_group, 'Coho|Chinook')) |> # filter to specific species
    # filter(str_detect(catch_group, 'Steelhead')) |> # filter to specific species
  filter(catch_group == "Steelhead_Adult_UM_Released") |> # filter to specific catch_group
  distinct() |>
  # filter(!is.na(mean_daily_TI_expan)) |> 
  arrange(catch_group, section, event_date, angler_type) |> 
  filter(!is.na(mean_daily_TI_expan))

# weekly_catch <- estimates$daily_effort_cpue_catch |>
#   group_by(section, time_strata, angler_type, catch_group) |>
#   summarize(
#     total_catch = sum(catch_dailysum),
#     total_hours = sum(total_angler_hours),
#     cpue_weekly = total_catch / total_hours
#   ) |>
#   mutate(
#     cpue_weekly = if_else(is.nan(cpue_weekly), 0, cpue_weekly)
#   ) |>
#   ungroup()

write_csv(estimates$daily_effort_cpue_catch, paste(model_output, paste(paste(params$proj_name, params$water_body, sep = "_"), paste("Point_Estimate", "Summary_Daily_Effort_Catch_CPUE", sep = "_"), paste(params$date_start, params$date_end, sep = "_"), ".csv", sep="_"), sep="/"))
          
```



# time period specific estimates 

Need to work on making the model period a script parameter so the user can easily swap out time period strata among week, month, bi-month, etc. 
Made progress on this on 3/10/2022
## effort by time period strata 
```{r estimates$time_strata_effort_total, message = FALSE, warning = FALSE,results = "asis",include=TRUE, echo=FALSE}

# create table with degrees of freedom by section and angler type to apply to time strata estimates

point_estimate_data_prelim$degrees_freedom <- estimates$daily_effort_cpue_catch |>
  distinct(section, angler_type, time_strata, n_days) |> 
  group_by(section, angler_type) |> 
  summarize(
    min_n_days = min(n_days),
    sum_n_days = sum(n_days),
    degrees_freedom = min_n_days + sum_n_days / 2
  )

# dates object to join back to summary objects to plotting 
estimates$days_join_2 <- d_days |> 
  group_by(Week, Month) |>
  slice(1) |>
  ungroup() |> 
  distinct(Week, .keep_all = TRUE) |> 
  rename(time_strata = params$model_period)

# calculate effort by day type and angler type strata

estimates$time_strata_effort_by_daytype <- estimates$daily_effort_cpue_catch |>
  group_by(section, angler_type, time_strata, DayType, n_days, N_days) |> 
  summarize(
    n = n(),
    sum_daily_effort_sampled_days = sum(mean_daily_TI_expan),
    mean_daily_effort = mean(mean_daily_TI_expan),
    variance_daily_effort = var(mean_daily_TI_expan),
    total_effort = mean_daily_effort * N_days,
    variance_total_daily_effort = if_else(n_days < N_days, (N_days^2) * (variance_daily_effort / n_days) * (1-(n_days/N_days)),  (N_days^2) * (variance_daily_effort / n_days))) |> 
  distinct()

# Sum effort estimates across day types, retain grouping on angler type 

estimates$time_strata_effort_total <- estimates$time_strata_effort_by_daytype |>
  ungroup() |>
 # add degrees of freedom per angler_type and section for time_strata
  # left_join(lut$sections |> select(section, section_name),
  #           by = "section") |>
  group_by(section, time_strata, angler_type) |> 
  summarise(
    total_effort = sum(total_effort),
    variance = sum(variance_total_daily_effort)) |> 
 left_join(point_estimate_data_prelim$degrees_freedom, by = c("section", "angler_type")) |> 
  mutate(
    variance = if_else(is.na(variance),0,variance),
    SE = sqrt(variance),
    CV = SE / total_effort,
    lwr95CI = total_effort - qt(1-(0.05/2),degrees_freedom)*(variance^0.5),
    upr95CI = total_effort + qt(1-(0.05/2),degrees_freedom)*(variance^0.5)
  ) |> 
  distinct(time_strata, angler_type, .keep_all = TRUE)|> 
  mutate_if(is.numeric, round, 2) |> 
  filter(angler_type %in% c("bank","boat")) |>
  left_join(estimates$days_join_2, by = "time_strata") 


total_effort <- estimates$time_strata_effort_total |> 
  filter(!is.na(total_effort)) |> 
  ungroup() |> 
  summarize(
    total_effort = sum(total_effort)
  )

# weekly_test <- estimates$time_strata_effort_total |>
#   left_join(weekly_catch) |>
#   mutate(
#     weekly_catch_estimate = cpue_weekly * total_effort
#   )
# 
# sum(weekly_test$weekly_catch_estimate)

write_csv(estimates$time_strata_effort_total, paste(model_output, paste(paste(params$proj_name, params$water_body, sep = "_"), paste("Point_Estimate", "Summary_Effort", params$model_period, sep = "_"), paste(params$date_start, params$date_end, sep = "_"), ".csv", sep="_"), sep="/"))

# time_strata_effort_total_table <- estimates$time_strata_effort_total |>
#   kbl(caption = "Total angler effort statistics in angler hours for boat and shore based fishing during the 2021 Skagit River fall salmon fishery from 08/14/2021 - 12/31/2021.",digits =2) |> 
#   # kable_classic(full_width = F, html_font = "Cambria") |> 
#   kable_styling(bootstrap_options = c("striped", "hover"))
# 
# print(time_strata_effort_total_table)

```

```{r, include=TRUE, fig.align="center",fig.width= 14, fig.height= 8, fig.cap=c("Figure 1. ____ effort estimates (angler hours) during the 2021 Skagit River fall salmon fishery from 08/14/2021 - 12/31/2021. Error bars show 95 percent confidence intervals."),echo=FALSE,warning = FALSE}
options(scipen=10000)


estimates$time_strata_effort_total |>
  # arrange(section) |> 
  # mutate(name=factor(section), levels=section) |> 
  # filter(!grepl(c("Steelhead"),Catch_Group)) |> 
  ggplot(aes(x = time_strata, y = total_effort, fill = angler_type)) +
  geom_bar(position = "dodge", stat = "identity", color = "black") +
  scale_fill_manual(values=c("#999999", "#E69F00", "lightblue")) +
  geom_errorbar(aes(ymin=lwr95CI, ymax=upr95CI), width=.2,
                position=position_dodge(.9)) +
  theme(legend.text=element_text(size=8)) +
  theme_bw(base_size = 12) + 
  facet_wrap(. ~ section + angler_type, ncol = 2)

```



## catch by time period strata 
```{r estimates$time_strata_catch_total, message = FALSE, warning = FALSE,results = "asis",include=TRUE, echo=FALSE}

# calculate catch by day type and angler type strata
estimates$time_strata_catch_by_daytype <- estimates$daily_effort_cpue_catch |>
  filter(!is.na(catch_estimate)) |> 
  group_by(section, angler_type, catch_group, time_strata, DayType, n_days, N_days) |> 
  summarize(
    total_catch_unexpanded = sum(catch_estimate),
    mean_daily_catch = total_catch_unexpanded / n_days,
    total_catch_expanded_2 = total_catch_unexpanded * N_days,
    total_catch_expanded = mean_daily_catch * N_days,
    variance_catch_estimate = var(catch_estimate),
    n_days = mean(n_days),
    N_days = mean(N_days)) |> 
  distinct() |> 
  mutate(
    variance_catch_estimate = replace_na(variance_catch_estimate, 0),
    variance_total_catch_expanded = if_else(n_days < N_days, (N_days^2) * (variance_catch_estimate / n_days) * (1-(n_days/N_days)),  (N_days^2) * (variance_catch_estimate / n_days))
  ) |> 
  distinct()

# cannot calculate variance for estimates when n = 1 for daytype strata within a week

# Sum catch estimates across day types, retain grouping on angler type 

estimates$time_strata_catch_total <- estimates$time_strata_catch_by_daytype |>
  ungroup() |>
 # add degrees of freedom per angler_type and section for time_strata
  # left_join(lut$sections |> select(section, section_name),
  #           by = "section") |> 
  group_by(section, time_strata, catch_group, angler_type) |> 
  summarise(
    total_catch = sum(total_catch_expanded),
    variance = sum(variance_total_catch_expanded)) |> 
    left_join(point_estimate_data_prelim$degrees_freedom, by = c("section", "angler_type")) |>
  mutate(
    variance = if_else(is.na(variance), 0, variance),
    SE = sqrt(variance),
    CV = SE / total_catch,
    lwr95CI = total_catch - qt(1-(0.05/2),degrees_freedom)*(variance^0.5),
    upr95CI = total_catch + qt(1-(0.05/2),degrees_freedom)*(variance^0.5)
  ) |>
  distinct() |> 
  mutate_if(is.numeric, round, 2) |> 
  filter(angler_type %in% c("bank", "boat")) |> 
  arrange(section, catch_group, time_strata) |>
  left_join(estimates$days_join_2, by = "time_strata") 


total_catch <- estimates$time_strata_catch_total |> 
  filter(!is.na(total_catch)) |> 
  ungroup() |>
  group_by(section, catch_group) |> 
  summarize(
    total_catch = sum(total_catch)
  )


write_csv(estimates$time_strata_catch_total, paste(model_output, paste(paste(params$proj_name, params$water_body, sep = "_"), paste("Point_Estimate", "Summary_Catch", params$model_period, sep = "_"), paste(params$date_start, params$date_end, sep = "_"), ".csv", sep="_"), sep="/"))


# time_strata_catch_total_table <- estimates$time_strata_catch_total |>
#   kbl(caption = "___ catch statistics for coho and Chinook during the 2021 Skagit River fall salmon fishery from 08/14/2021 - 12/31/2021, combing both bank and boat angler types.",digits =2) |> 
#   # kable_classic(full_width = F, html_font = "Cambria") |> 
#   kable_styling(bootstrap_options = c("striped", "hover"))

# print(time_strata_catch_total_table)

```



```{r, include=TRUE, fig.align="center",fig.width= 14, fig.height= 8, fig.cap=c("Figure 1. ___ harvest estimates for ad-clipped and unmarked coho during the 2021 Skagit River fall salmon fishery from 08/14/2021 - 12/31/2021. Error bars show 95 percent confidence intervals."),echo=FALSE,warning = FALSE}

estimates$time_strata_catch_total |>
  # filter(catch_group %in% c("Chinook_Adult_UM_Released", "Chinook_Adult_AD_Released")) |>
    filter(catch_group %in% c("Steelhead_Adult_UM_Released")) |>
  # rename(Week = time_strata) |>
  # left_join(d_days, by = "Week") |>
  # left_join(lut$sections |>
  #             select(section, section_name)) |>
  # distinct(total_catch, lwr95CI, upr95CI, angler_type, event_date, section, section_name, Week, Month) |>
  # group_by(section, angler_type, Week, Month) |>
  # slice(1) |>
  # mutate(
  #   Week = lubridate::week(event_date),
  #   month_numeric = as.numeric(format(Month, format = "%U")),
  #   month_label = format(Month, format = "%b")) |> 
  ggplot(aes(x = time_strata, y = total_catch, fill = as.factor(section))) +
  geom_bar(position = "dodge", stat = "identity") +
  scale_fill_manual(values=c("#999999", "#E69F00", "blue", "brown", "grey")) +
  geom_errorbar(aes(ymin=lwr95CI, ymax=upr95CI), width=.2,
                position=position_dodge(.9)) +
  theme(legend.text=element_text(size=8)) +
  theme_bw(base_size = 12) +
  # scale_x_date() +
  facet_wrap(. ~ catch_group + angler_type + section)

```



## CPUE by time period strata  
```{r, inclue = FALSE}

# EB circle back to this for calculating CPUE / HPUE and associated uncertainty over time period of interest 

estimates$effort_rename <- estimates$time_strata_effort_total |> 
              select(section,
                     time_strata,
                     total_effort,
                     angler_type,
                     effort_lwr95CI = lwr95CI,
                     effort_upr95CI = upr95CI,
                     effort_SE = SE,
                     effort_CV = CV)

# estimates$cpue_time_strata <- estimates$time_strata_catch_total  |>
#   select(
#     section,
#     time_strata,
#     angler_type,
#     total_catch,
#     catch_variance = variance,
#     catch_SE = SE,
#     catch_CV = CV,
#     catch_lwr95CI = lwr95CI,
#     catch_upr95CI = upr95CI
#   ) |>
#   mutate(
#     daily_catch_estimate = total_catch / 7
#   ) |> 
#   left_join(effort_rename, by = c("section","angler_type","time_strata")) |> 
#   mutate_if(is.numeric, round, 2) |> 
#   mutate(
#     cpue_time_strata = replace_na(total_catch / total_effort, 0),
#     cpue_lwr95_time_strata = catch_lwr95CI / effort_lwr95CI,
#     cpue_upr95_time_strata = catch_upr95CI / effort_upr95CI,
#     cpue_lwr95_time_strata = if_else(is.na(cpue_lwr95_time_strata),0, cpue_lwr95_time_strata),
#     cpue_upr95_time_strata = if_else(is.na(cpue_upr95_time_strata),0, cpue_upr95_time_strata)) |> 
#   distinct() 



estimates$cpue_time_strata <- estimates$time_strata_catch_total  |>
  select(
    section,
    time_strata,
    angler_type,
    total_catch,
    catch_variance = variance,
    catch_SE = SE,
    catch_CV = CV,
    catch_lwr95CI = lwr95CI,
    catch_upr95CI = upr95CI
  ) |>
  mutate(
    daily_catch_estimate = total_catch / 7
  ) |> 
  left_join(estimates$effort_rename, by = c("section","angler_type","time_strata")) |> 
  mutate_if(is.numeric, round, 2) |> 
  mutate(
    cpue_time_strata = replace_na(total_catch / total_effort, 0),
    cpue_lwr95_time_strata = catch_lwr95CI / effort_lwr95CI,
    cpue_upr95_time_strata = catch_upr95CI / effort_upr95CI,
    cpue_lwr95_time_strata = if_else(is.na(cpue_lwr95_time_strata),0, cpue_lwr95_time_strata),
    cpue_upr95_time_strata = if_else(is.na(cpue_upr95_time_strata),0, cpue_upr95_time_strata)) |> 
  distinct() |>
  left_join(estimates$days_join_2, by = "time_strata") 



write_csv(estimates$cpue_time_strata, paste(model_output, paste(paste(params$proj_name, params$water_body, sep = "_"), paste("Point_Estimate", "Catch_CPUE_time_strata", params$model_period, sep = "_"), paste(params$date_start, params$date_end, sep = "_"), ".csv", sep="_"), sep="/"))


# results$CPUE |> 
#   ggplot(aes(x = event_date, y = `50%`, fill = angler_type, color = angler_type)) +
#   geom_ribbon(aes(ymin = `2.5%`, ymax = `97.5%`), alpha = 0.5) +
#   geom_line() + geom_point() +
#   scale_x_date() +
#   facet_wrap(~section_name + angler_type, ncol = 2)


estimates$cpue_time_strata |> 
  filter(catch_group %in% c( "Steelhead_Adult_UM_Released")) |> 
  ggplot(aes(x = event_date, y = total_catch, fill = angler_type)) +
  geom_ribbon(aes(ymin = catch_lwr95CI, ymax = catch_upr95CI, group = angler_type)) +
  geom_line() +
  scale_x_date() +
  # geom_point(position = "dodge", stat = "identity", color = "black") +
  scale_fill_manual(values=c("steelblue", "#E69F00")) +
  # geom_errorbar(aes(ymin= cpue_lwr95_time_strata , ymax= cpue_upr95_time_strata), width=.2,
  #               position=position_dodge(.9)) +
  # theme(legend.text=element_text(size=8)) +
  theme_bw(base_size = 12) +
  facet_wrap(. ~ catch_group + section + angler_type, nrow = 3)

# estimates$cpue_time_strata |> 
#   filter(catch_group %in% c("Chinook_Adult_UNK_Released")) |> 
#   ggplot(aes(x = time_strata, y = total_catch, fill = angler_type)) +
#   geom_ribbon(aes(ymin = catch_lwr95CI, ymax = catch_upr95CI, group = angler_type)) +
#   geom_line() +
#   # scale_x_date() +
#   # geom_point(position = "dodge", stat = "identity", color = "black") +
#   scale_fill_manual(values=c("steelblue", "#E69F00")) +
#   # geom_errorbar(aes(ymin= cpue_lwr95_time_strata , ymax= cpue_upr95_time_strata), width=.2,
#   #               position=position_dodge(.9)) +
#   # theme(legend.text=element_text(size=8)) +
#   theme_bw(base_size = 12) +
#   facet_wrap(. ~ catch_group + section + angler_type, nrow = 3)
# 
# estimates$cpue_time_strata |> 
#   filter(catch_group %in% c("Chinook_Adult_AD_Released")) |> 
#   ggplot(aes(x = time_strata, y = total_catch, fill = angler_type)) +
#   geom_ribbon(aes(ymin = catch_lwr95CI, ymax = catch_upr95CI, group = angler_type)) +
#   geom_line() +
#   # scale_x_date() +
#   # geom_point(position = "dodge", stat = "identity", color = "black") +
#   scale_fill_manual(values=c("steelblue", "#E69F00")) +
#   # geom_errorbar(aes(ymin= cpue_lwr95_time_strata , ymax= cpue_upr95_time_strata), width=.2,
#   #               position=position_dodge(.9)) +
#   # theme(legend.text=element_text(size=8)) +
#   theme_bw(base_size = 12) +
#   facet_wrap(. ~ catch_group + section + angler_type, nrow = 3)

```


# total estimates across monitoring period

## degrees of freedom for total estimates 
```{r}
# create table with degrees of freedom by section and angler type to apply to estimates across all days (total)

point_estimate_data_prelim$degrees_freedom_total <- point_estimate_data_prelim$effort_interviews_final |> 
  distinct(section, angler_type, n_days_total) |>
  rename(n_days = n_days_total) |> 
  group_by(section, angler_type) |> 
  summarize(
    min_n_days = min(n_days),
    sum_n_days = sum(n_days),
    degrees_freedom = min_n_days + sum_n_days / 2
  )

```


## total effort across monitoring period
```{r estimates$time_strata_effort_total, message = FALSE, warning = FALSE,results = "asis",include=TRUE, echo=FALSE}

# Adding together time strata specific effort estimates and associated variance and calculating SE, CV, and 95% CI's
estimates$total_effort <- estimates$time_strata_effort_total |>
  select(-c(min_n_days, sum_n_days, degrees_freedom)) |> 
  left_join(point_estimate_data_prelim$degrees_freedom_total, by = c("section", "angler_type")) |> 
  group_by(section, angler_type) |> 
  summarise(
    total_effort = sum(total_effort),
    variance = sum(variance),
    SE = sqrt(variance),
    CV = SE / total_effort,
    lwr95CI = total_effort - qt(1-(0.05/2),degrees_freedom)*(variance^0.5),
    upr95CI = total_effort + qt(1-(0.05/2),degrees_freedom)*(variance^0.5)
  ) |>
  distinct() |> 
  mutate_if(is.numeric, round, 2) |> 
  arrange(section)


write_csv(estimates$total_effort, paste(model_output, paste(paste(params$proj_name, params$water_body, sep = "_"), paste("Point_Estimate", "Summary_Total_Effort", sep = "_"), paste(params$date_start, params$date_end, sep = "_"), ".csv", sep="_"), sep="/"))

total_effort <- estimates$total_effort |>
  ungroup() |> 
  summarize(
    total_effort = sum(total_effort)
  )

total_effort_table <- estimates$total_effort |>
  filter(angler_type == "total") |> 
  kbl(caption = "Total angler effort statistics in angler hours for boat and shore based fishing during the 2021 Skagit River fall salmon fishery from 08/14/2021 - 12/31/2021.",digits =2) |> 
  # kable_classic(full_width = F, html_font = "Cambria") |> 
  kable_styling(bootstrap_options = c("striped", "hover"))

print(total_effort_table)

```

```{r, include=TRUE, fig.align="center",fig.width= 14, fig.height= 8, fig.cap=c("Figure 1. _____ effort estimates (angler hours) during the 2021 Skagit River fall salmon fishery from 08/14/2021 - 12/31/2021. Error bars show 95 percent confidence intervals."),echo=FALSE,warning = FALSE}
options(scipen=10000)

estimates$total_effort |>
  # arrange(section) |> 
  # mutate(name=factor(section), levels=section) |> 
  # filter(!grepl(c("Steelhead"),Catch_Group)) |> 
  ggplot(aes(x = angler_type, y = total_effort)) +
  geom_bar(position = "dodge", stat = "identity", color = "black") +
  scale_fill_manual(values=c("#999999", "#E69F00")) +
  geom_errorbar(aes(ymin=lwr95CI, ymax=upr95CI), width=.2,
                position=position_dodge(.9)) +
  theme(legend.text=element_text(size=8)) +
  theme_bw(base_size = 12) + 
  facet_wrap(. ~ section, ncol = 2)

```



## total catch across monitoring period 
```{r estimates$time_strata_catch_total, message = FALSE, warning = FALSE,results = "asis",include=TRUE, echo=FALSE}

# Adding together time_strata catch estimates and associated variance and calculating SE, CV, and 95% CI's

estimates$total_catch <- estimates$time_strata_catch_total |>
  select(-c(min_n_days, sum_n_days, degrees_freedom)) |> 
  left_join(point_estimate_data_prelim$degrees_freedom_total, by = c("section", "angler_type")) |> 
  mutate(variance = replace_na(variance, 0)) |> 
  group_by(catch_group, section, angler_type) |> 
  summarise(
    total_catch = sum(total_catch),
    variance = sum(variance),
    SE = sqrt(variance),
    CV = SE / total_catch,
    lwr95CI = total_catch - qt(1-(0.05/2),degrees_freedom)*(variance^0.5),
    upr95CI = total_catch + qt(1-(0.05/2),degrees_freedom)*(variance^0.5)
  ) |>
  distinct() |> 
  mutate_if(is.numeric, round, 2) |> 
  filter(angler_type %in% c("bank", "boat")) |> 
  arrange(section, catch_group)


view(estimates$total_catch)

write_csv(estimates$total_catch, paste(model_output, paste(paste(params$proj_name, params$water_body, sep = "_"), paste("Point_Estimate", "Summary_Total_Catch", sep = "_"), paste(params$date_start, params$date_end, sep = "_"), ".csv", sep="_"), sep="/"))

total_catch_totals <- estimates$total_catch |> 
  group_by(catch_group) |> 
  summarise(
    total_catch = sum(total_catch)
  )


total_catch_table <- estimates$total_catch |>
  filter(angler_type == "total") |> 
  kbl(caption = "Total catch statistics for coho and Chinook during the 2021 Skagit River fall salmon fishery from 08/14/2021 - 12/31/2021, combing both bank and boat angler types.",digits =2) |>
  kable_styling(bootstrap_options = c("striped", "hover"))

print(total_catch_table)

```




```{r, include=TRUE, fig.align="center",fig.width= 14, fig.height= 8, fig.cap=c("Figure 1. ____ harvest estimates for ad-clipped and unmarked coho during the 2021 Skagit River fall salmon fishery from 08/14/2021 - 12/31/2021. Error bars show 95 percent confidence intervals."),echo=FALSE,warning = FALSE}

estimates$total_catch |> 
  # filter(!grepl(c("Steelhead"),Catch_Group)) |>
  filter(catch_group %in% c("Steelhead_Adult_UM_Released")) |>
  ggplot(aes(x = catch_group, y = total_catch)) +
  geom_bar(position = "dodge", stat = "identity", color = "black") +
  scale_fill_manual(values=c("#999999", "#E69F00")) +
  geom_errorbar(aes(ymin=lwr95CI, ymax=upr95CI), width=.2,
                position=position_dodge(.9)) +
  theme(legend.text=element_text(size=8)) +
  theme_bw(base_size = 12) + 
  facet_wrap(. ~ section)

estimates$total_catch |> 
  # filter(!grepl(c("Steelhead"),Catch_Group)) |> 
  filter(angler_type == "total" & catch_group %in% c("Chinook_Adult_UM_Released")) |>
  ggplot(aes(x = catch_group, y = total_catch)) +
  geom_bar(position = "dodge", stat = "identity", color = "black") +
  scale_fill_manual(values=c("#999999", "#E69F00")) +
  geom_errorbar(aes(ymin=lwr95CI, ymax=upr95CI), width=.2,
                position=position_dodge(.9)) +
  theme(legend.text=element_text(size=8)) +
  theme_bw(base_size = 12) + 
  facet_wrap(. ~ section)

```