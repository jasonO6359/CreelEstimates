---
title: "FW Creel Point Estimate"
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: scroll
    source_code: embed
    theme:
      version: 4
      bootswatch: default
      bg: "#D0D0D0"
      fg: "#000000"
      navbar-bg: "#1189D9"
      primary: "#1189D9"
runtime: shiny
---

```{css}
  .chart-shim {
    overflow: auto;
    }
```

```{r setup, include=FALSE}
#bslib::bs_themer()

library("flexdashboard")
library("tidyverse")
library("shiny")
library("patchwork")
library("gt")

#base endpoints
dwg_base <- list(
  event = "https://data.wa.gov/resource/ui95-axtn.csv",
  effort = "https://data.wa.gov/resource/h9a6-g38s.csv",
  interview = "https://data.wa.gov/resource/rpax-ahqm.csv",
  catch = "https://data.wa.gov/resource/6y4e-8ftk.csv",
  gear = "https://data.wa.gov/resource/d2ks-afhz.csv" #currently unused?
)

#lookup objects created in build_envi chunk below: 
# -dates of holidays
# -river lon/lat (for day length)
# -sections (for aggregating count/interview locations)
# -tie-in expansions
# -closure files
load("fw_creel_dash_envi.RData")

#currently reliant on consistent naming practice across lu-type
#could revise to coerce, but okay to allow failure with mismatched names
#names(lu$census_expansions); names(lu$closures_by_section)
input_analysis_levels <- as_tibble(
  cbind(names(lu$sections), str_split_fixed(names(lu$sections), pattern = "_", n = 6)),
  .name_repair = ~c("analysis_name", "proj_name", "water_body", "season", "species", "year", "misc")
  )

lu_input <- reactiveValues() #focal river_lonlat, sections, census_exp and closures
creel <- reactiveValues() #data queried from DWG as list of tibbles
pe <- reactiveValues() #intermediate objects wrangled from creel list elements

```

```{r build_envi, eval=FALSE}
# # uncomment and run chunk to rebuild envi of lookups at dashboard load...
# # need to build out route to load sections/closures etc on the fly?
# # at some point the environment file size gets overlarge? or not realistically a near-term issue?


# lu <- list()
# 
# lu$dates_holidays_2015_2030 <- read_lines("input_files/dates_holidays_2015_2030.txt") |>
#   as.Date(format="%Y-%m-%d")
# 
# #single tibble
# lu$river_lonlat <- readr::read_csv("input_files/river_locations_master.csv")
# 
# #list of tibbles
# lu$sections <- list.files("input_files", pattern = "_sections.csv", full.names = T) |>
#   set_names() |> map(readr::read_csv)
# names(lu$sections) <- tools::file_path_sans_ext(basename(names(lu$sections))) |> str_remove("_sections")
# 
# #list of tibbles
# lu$census_expansions <- list.files("input_files", pattern = "_tiexp.csv", full.names = T) |>
#   set_names() |> map(readr::read_csv)
# names(lu$census_expansions) <- tools::file_path_sans_ext(basename(names(lu$census_expansions))) |> str_remove("_tiexp")
# 
# # # #!! snippet to mod to build closure.csvs for other datasets...
# # # #!! change vals in event_date, s and write_csv
# # # #!! file is date-complete over the focal range
# # # #!! with cols per section, elements coded open==T, closed==F
# # expand_grid(
# #   event_date = seq.Date(as.Date("2021-09-16"), as.Date("2021-11-30"), by = "day"),
# #   s = paste0("open_section_", c(1)),
# #   is_open = TRUE
# #   ) |>
# #   pivot_wider(names_from = s, values_from = is_open) |>
# #   write_csv("input_files/district_river_season_species_year_closures.csv")
# 
# #list of tibbles
# lu$closures_by_section <- list.files("input_files", pattern = "_closures.csv", full.names = T) |>
#   set_names() |> map(readr::read_csv)
# names(lu$closures_by_section) <- tools::file_path_sans_ext(basename(names(lu$closures_by_section))) |> str_remove("_closures")
# 
# save.image("fw_creel_dash_envi.RData")

```

```{r gt_catch, echo=FALSE}
gt_catch <- function(cg_string, negate = F){
  if(negate){
    d <- filter(pe$est_catch_s_ts_dt_at, !str_detect(catch_group, cg_string))
  } else {
    d <- filter(pe$est_catch_s_ts_dt_at, str_detect(catch_group, cg_string))
  }
  if(nrow(d)>0){
    bind_rows(
      d |> group_by(section, DayType, angler_type, catch_group) |> 
        summarise(across(c(est, var), ~round(sum(.), 1)), .groups = "drop")
      ,
      d |> group_by(section, catch_group) |> 
        summarise(
          angler_type = "total", DayType = "total",
          est = round(sum(est), 1), .groups = "drop")
    ) |> 
      gt(groupname_col = c("section","catch_group")) |> 
      gt::fmt_number(columns = where(is.numeric), decimals = 1) |> 
      gt::sub_missing(columns = where(is.numeric)) |>
      gt::tab_style(
        style = gt::cell_text(weight = "bold"),
        locations = cells_body(columns = starts_with("est"), rows = DayType == "total")
      ) |> 
      gt::tab_style(
        style = gt::cell_fill(color = "#fc6508", alpha = 0.3),
        locations = cells_body(columns = contains("est"), rows = var/est > 5)
      ) |>  
      gt::tab_style(
        style = gt::cell_borders(sides = "left"),
        locations = cells_body(columns = starts_with("est"), rows = DayType != "total")
      ) |> 
      gt::tab_options(container.overflow.x = TRUE, container.overflow.y = TRUE)
  }
}

```


Sidebar {.sidebar data-width=350}
============================================

### Controls

```{r input_selectors}
selectInput(inputId = 'analysis_name', label = 'Analysis Name', choices = input_analysis_levels$analysis_name, selected = input_analysis_levels$analysis_name[1])


radioButtons(inputId = 'model_period', label = "Model period",
             choices = c("Week", "Month", "Duration"), inline = T,
             selected = "Week"
             )

radioButtons(inputId = 'index_count_types', label = "Index count types", 
             choices = c("Bank/Boat Anglers", "Vehicle/Trailers Only")
             ,selected = c("Vehicle/Trailers Only")
             )

radioButtons(inputId = 'census_expansion', label = "Census expansion method", inline = T,
             choices = c("Direct", "Indirect")
             ,selected = c("Direct")
             )

checkboxGroupInput(inputId = 'days_wkend', label = "Days in 'weekends'",
                   choiceNames =  c("Mon", "Tues", "Wed", "Thur", "Fri", "Sat", "Sun"),
                   choiceValues = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"),
                   selected = c("Saturday", "Sunday"))

numericInput(inputId = 'int_ang_hrs_tot_min', label = "Minimum interview total angler hours", 
             value = 0.25, min = 0, max = 2)

# #can add option to manually upload ...
# fileInput(inputId = 'sections', label = 'Sections LU', accept = ".csv")

```

### Fetch

```{r dwg_fetch}
#fires the requests on button push
actionButton("fetch", label = "Fetch raw data")


# ## uncomment for console dev
# lu_input <- list()
# input <- list()
# creel <- list()
# pe <- list()
# 
# input$analysis_name <- input_analysis_levels$analysis_name[5]
# 
# input$model_period <- "Week"
# # input$model_period <- "Month"
# input$index_count_types <- "Vehicle/Trailers Only"
# input$census_expansion <- "Direct"
# input$days_wkend <- c("Friday","Saturday", "Sunday")
# # input$days_wkend <- c("Saturday", "Sunday")
# input$int_ang_hrs_tot_min <- 0.5


#this is "event reactive", so should not trigger on fiddling with inputs
#but only when "fetch" button is clicked (see last few lines of chunk)
reactive({
  
  input_analysis <- dplyr::filter(input_analysis_levels, analysis_name == input$analysis_name)

  #!!Lon/lat only used in day-length calc?
  #!!anywhere/anywhy that distinct() would be a problem?
  #!!i.e., anyone realistically using this to run ests on widely spaced rivers?
  lu_input$river_lonlat <- dplyr::filter(
    lu$river_lonlat, 
    River %in% input_analysis$water_body #River %in% input$water_body
    ) |> 
    dplyr::distinct(River, .keep_all = T)
  
  #!!note the current fetch drops/excludes effort and interview rows
  #!!if no section assigned from section LU
  lu_input$sections <- dplyr::select(
    lu$sections[[input$analysis_name]], #lu$sections[[input$sections]],
    water_body_desc, location = location_code, section)
  
  lu_input$census_exp <- dplyr::mutate(
    lu$census_expansions[[input$analysis_name]],
    cen_exp_meth = input$census_expansion)

  lu_input$closures <- tidyr::drop_na(lu$closures_by_section[[input$analysis_name]], event_date)

  
  creel$event <- paste0(
    dwg_base$event,
    "?$where=project_name in('", input_analysis$proj_name, "')",
    " AND water_body in('", paste0(input_analysis$water_body, collapse = "','"), "')", 
    " AND event_date between '", first(lu_input$closures$event_date),
    "T00:00:00' and '", last(lu_input$closures$event_date),
    "T00:00:00'&$limit=100000"
    ) |>
    utils::URLencode() |>
    readr::read_csv(show_col_types = F) |>
    dplyr::select(creel_event_id, water_body, event_date, tie_in_indicator)
  
  if(nrow(creel$event) > 0) {
    
    creel$effort <- paste0(
      dwg_base$effort,
      "?$where=creel_event_id in('",
      paste(creel$event$creel_event_id, collapse = "','"),
      "')&$limit=100000"
      ) |> 
      utils::URLencode() |>
      readr::read_csv(show_col_types = F) |>
      dplyr::filter(!is.na(count_type)) |> 
      dplyr::select(-created_datetime, -modified_datetime) |> 
      dplyr::inner_join(lu_input$sections, by = c("location"))

    creel$interview <- paste0(
      dwg_base$interview,
      "?$where=creel_event_id in('",
      paste(creel$event$creel_event_id, collapse = "','"),
      "')&$limit=100000"
      ) |> 
      utils::URLencode() |>
      readr::read_csv(show_col_types = F) |>
      dplyr::select(
        -created_datetime, -modified_datetime,
        -state_residence, -zip_code) |> 
      dplyr::mutate(
        location = if_else(is.na(interview_location), as.character(fishing_location), as.character(interview_location))
      ) |> 
      dplyr::inner_join(lu_input$sections, by = c("location"))
    
    #this needs to be filtered by the date-section applicable interviews or otherwise contains catch across entire project!
    creel$catch <- paste0(
      dwg_base$catch,
      "?$where=creel_event_id in('",
      paste(creel$event$creel_event_id, collapse = "','"),"')",
      " AND interview_id in('", 
      paste(creel$interview$interview_id, collapse = "','"),
      "')&$limit=100000"
      ) |> 
      utils::URLencode() |>
      readr::read_csv(show_col_types = F) |>
      dplyr::select(interview_id, catch_id, species, run, life_stage, fin_mark, fate, fish_count) |> 
      dplyr::mutate(
        catch_group = paste(species, life_stage, fin_mark, fate, sep = "_") # fish catch groups to estimate catch of 
      )
    
    #this whole chunk depends on button push, so "lazy" against fiddling
    creel$days <- tibble::tibble(
      event_date = lu_input$closures$event_date,
      Day = weekdays(event_date),
      DayType = if_else(
        Day %in% input$days_wkend | Day %in% lu$dates_holidays_2015_2030,
        "Weekend", "Weekday"),
      DayType_num = if_else(str_detect(DayType, "end"),1,0),
      DayL = suncalc::getSunlightTimes(
        date = event_date,
        tz = "America/Los_Angeles",
         lat = lu_input$river_lonlat$Lat,
         lon = lu_input$river_lonlat$Long,
        keep=c("sunrise", "sunset")
        ) |>
        mutate(DayL = as.numeric((sunset + 3600) - (sunrise - 3600))) |>
        pluck("DayL"),
      #Monday to Sunday weeks, see ?strptime
      Week = Week = as.numeric(format(event_date, "%W")),
      Month = as.numeric(format(event_date, "%m")),
      ModelPeriod = input$model_period,
      time_strata = case_when(
        ModelPeriod == "Week" ~ Week,
        ModelPeriod == "Month" ~ Month,
        ModelPeriod == "Duration" ~ double(1)
      )
    ) |>
      tibble::rowid_to_column(var = "day_index") |>
      dplyr::left_join(lu_input$closures, by = "event_date")
    
    # excluding specified closures, total number of days by section, weekday/end, and time strata for which to generate estimates
    creel$days_total <- creel$days |>
      pivot_longer(
        cols = starts_with("open_section"), 
        names_to = "section", 
        values_to = "is_open") |>
      filter(is_open) |> 
      mutate(section = as.numeric(gsub("^.*_", "", section))) |>
      count(time_strata, DayType, section, name = "N_days")
    
  } else {
    creel$effort <- NULL
    creel$interview <- NULL
    creel$catch <- NULL
  }
}) |> 
  bindEvent(input$fetch)
  
```

### Estimate

```{r calc_pe_ests}
actionButton("estimate", label = "Calculate estimates")

reactive({
  #### pe$effort_census --------------
  #Aggregate census (tie in) effort counts, associating to closest-in-time index count.
  #take the initial effort_census, with all count_sequence == 1,
  #add/overwrite the count_sequence val with that from closest temporal match from inline/anonymous paired counts object
  pe$effort_census <- dplyr::left_join(
    #begin with focal census data
    creel$effort |> 
      dplyr::filter(tie_in_indicator == 1) |>
      dplyr::select(section, location, event_date, tie_in_indicator, count_type, count_quantity)
    ,
    #reassign count_seq from closest index
    dplyr::left_join(
      creel$effort |> dplyr::filter(tie_in_indicator == 1) |> 
        dplyr::distinct(section, location, event_date, tie_in_indicator, effort_start_time, count_sequence),
      creel$effort |> dplyr::filter(tie_in_indicator == 0) |> 
        dplyr::distinct(section, event_date, tie_in_indicator, effort_start_time, count_sequence),
      by = c( "section", "event_date"),
      suffix = c("_cen", "_ind")
      ) |>
      dplyr::group_by(section, event_date, location) |>
      dplyr::slice_min(abs(effort_start_time_cen - effort_start_time_ind), n = 1) |>
      dplyr::ungroup() |>
      dplyr::distinct(section, location, event_date, count_sequence = count_sequence_ind)
    ,
    by = c("section", "location", "event_date")
    ) |>
    dplyr::mutate(
      angler_type = dplyr::case_when(
        stringr::word(count_type, 1) %in% c("Bank","bank","Shore") ~ "bank",
        stringr::word(count_type, 1) %in% c("Boat","boat") ~ "boat" #Note "Boats" plural intentionally excluded
      )
    ) |>
    tidyr::drop_na(angler_type) |> 
    dplyr::group_by(section, event_date, tie_in_indicator, count_sequence, angler_type) |>
    dplyr::summarize(count_census = sum(count_quantity), .groups = "drop") |>
    dplyr::arrange(section, event_date, count_sequence)
  
  #### pe$effort_index --------------
  # filters and aggregates over locations within count_seq & section
  # mutate angler_type here creates a later join-by column
  pe$effort_index <- dplyr::filter(
    creel$effort, 
    tie_in_indicator == 0,
    is.na(no_count_reason),
    !is.na(count_type)
    ) |>
    dplyr::left_join(creel$days |> select(event_date, DayType), by = "event_date") |> 
    dplyr::group_by(section, DayType, event_date, count_sequence, count_type) |>
    dplyr::summarise(count_index = sum(count_quantity), .groups = "drop") |>
    dplyr::mutate(
      angler_type = dplyr::case_when(
        count_type == "Boat Anglers" ~ "boat",
        count_type == "Bank Anglers" ~ "bank",
        count_type == "Trailers Only" ~ "boat",
        count_type == "Vehicle Only" ~ "total"
      )
    ) |> 
    dplyr::arrange(section, event_date, count_sequence)
  
  #### pe$interview --------------
  # #interview data have angler_count, vehicle_count, trailer_count, angler_type and boat_used
  # #disallow NAs in angler_type, fill missing angler_type from boat_used
  pe$interview <- dplyr::left_join(
    creel$interview, 
    creel$days |> select(event_date, time_strata, DayType), 
    by = "event_date"
    ) |>
    dplyr::mutate(
      dplyr::across(c(vehicle_count, trailer_count), ~replace_na(., 0)),
      trip_status = replace_na(trip_status, "Unknown"),
      
      angler_type = tolower(angler_type),
      angler_type = dplyr::if_else(is.na(angler_type), boat_used, angler_type),
      angler_type = dplyr::case_when( 
        angler_type == "boat" ~ "boat", #pass through
        angler_type == "bank" ~ "bank", #pass through
        angler_type == "Unk" ~ "bank",  #Kalama, others?
        angler_type == "No" ~ "bank",   #value from boat_used
        angler_type == "Yes" ~ "boat"   #value from boat_used
      ),
      angler_type_ind = as.integer(factor(angler_type)),

      fishing_end_time = dplyr::if_else(
        trip_status == "Incomplete" | is.na(fishing_end_time),
        interview_time,
        fishing_end_time),
      angler_hours = round(as.numeric(fishing_end_time - fishing_start_time) / 3600, 5),
      angler_hours_total = angler_count * angler_hours
    ) |>
    dplyr::filter(angler_hours_total >= input$int_ang_hrs_tot_min) |> 
    #drop a few unused cols for ease of dev
    dplyr::select(-creel_event_id, -water_body, -project_name, -interview_number,
           -crc_area, -fishing_location, -ends_with("_time"),
           -previously_interviewed, -comment_txt, -water_body_desc
    ) |> 
    dplyr::arrange(section, time_strata, event_date, angler_type, location)
  
  #### pe$interview_and_catch --------------
  # join catch data to interview after inferring complete cases (counts or 0s for all catch_groups encountered)
  # note rows in creel$catch for "short" interviews are lost if filtered above 
  # interview_ids in creel$interview/pe_interview with no reported encounters are not in creel$catch
  # so the pivots ensure that no-encounter interviews get assigned 0 fish_count for each possible catch_group
  # and are therefore included in the later CPUE
  pe$interview_and_catch <- left_join(
    pe$interview |> 
      select(section, time_strata, DayType, event_date,
             interview_id, angler_type, ends_with("_count"), angler_hours, angler_hours_total)
    ,
    creel$catch |> 
      group_by(interview_id, catch_group) |> 
      summarise(fish_count = sum(fish_count, na.rm = T), .groups = "drop") |> 
      pivot_wider(names_from = catch_group, values_from = fish_count)
    ,
    by = "interview_id"
  ) |> 
    pivot_longer(cols = -c(section:angler_hours_total), names_to = "catch_group", values_to = "fish_count") |> 
    mutate(fish_count = replace_na(fish_count, 0))
  
  #### pe$angler_hours_daily_mean --------------
  # depending on the types of index counts, reach the calc: ang_hrs_daily_mean_TI_expan = angler_hours_daily_mean * TI_expan_final
  # when index counts are already bank & boat, matching census counts,
  #   then angler_hour daily means are just effort index counts of anglers expanded by day length,
  #   which are multiplied against tie-in expanded census counts of anglers by type (per section)
  # but when index counts are trailers & vehicles,
  #   then angler_hour daily means require first using interviews to estimate anglers_per_vhcl_trlr by angler_type total & boat 
  #   so anglers_per_vhcl_trlr can be multiplied against the trailer & vehicle counts in effort_index, releveled to boat/total
  #   and then TI-expanded counts similarly require splitting, releveling and rebinding census to boat/total to allow join with effort_index
  #   and THEN generating a final object with total, boat and derived-bank, including dealing with case of only-bank (e.g., Cascade)

  if(str_detect(input$index_count_types, "Bank|Boat")) {
    #initial angler_hours_daily_mean: join day length against mean counts over count_seqs per section-day-angler_type
    pe$angler_hours_daily_mean <- left_join(
      pe$effort_index |> 
        dplyr::group_by(section, event_date, angler_type) |>
        dplyr::summarise(count_index_mean = mean(count_index), .groups = "drop"),
      creel$days |> select(event_date, DayL),
      by = "event_date") |>
      mutate(angler_hours_daily_mean = count_index_mean * DayL) |>
      arrange(section, event_date)
    
    #census_TI_expan: left join effort index counts to census counts and expand by TI
    #count_census begins summed by event_date, section, tie_in_indicator, count_sequence, with angler_type in [bank, boat]
    #excluding date-section-anglers with missing (or negative) counts as invalid to support inferring point estimates
    pe$census_TI_expan <- left_join(
      pe$effort_census,
      pe$effort_index,
      by = c("section", "event_date", "count_sequence", "angler_type")
      ) |>
      filter(
        !is.na(count_census), !is.na(count_index),
        count_census >= 0, count_index >= 0
        ) |> 
      group_by(section, angler_type) |>
      summarise(
        across(c(count_census, count_index), sum),
        TI_expan_weighted = count_census / count_index,
        TI_expan_weighted = if_else(TI_expan_weighted == 0, 1, TI_expan_weighted) |> replace_na(1),
        .groups = "drop") |>
      left_join(
        lu_input$census_exp |>
          select(cen_exp_meth, angler_type, section, p_TI, TI_expan_indirect = Indirect_TI_Expan),
        by = c("section", "angler_type")) |>
      mutate(
        cen_exp_meth = replace_na(cen_exp_meth, input$census_expansion),
        p_TI = replace_na(p_TI, 1),
        TI_expan_indirect = as.numeric(TI_expan_indirect) |> replace_na(1),
        TI_expan_final = if_else(
          cen_exp_meth == "Direct",
          TI_expan_weighted / p_TI,
          TI_expan_indirect)
      )
    #now overwrite object, adding TI-expansions
    pe$angler_hours_daily_mean <- left_join(
      pe$angler_hours_daily_mean,
      pe$census_TI_expan,
      by = c("section", "angler_type")
      ) |>
      mutate(ang_hrs_daily_mean_TI_expan = angler_hours_daily_mean * replace_na(TI_expan_final, 1)) 

  } else if(str_detect(input$index_count_types, "Vehicle|Trailer")) {
    #in this case, boat and bank counts must be indirectly calc'd
    #first the interviews are used to estimate anglers per vehicle, stratified by boat vs total (from boat|bank angler_type rows in interview)
    #note that further below the section-dayType-angler_type mean is used to infer missing values 
    #both where a given section-date has interviews but lacks an angler_type
    #and where a section-date index count is missing interview data for either/both angler_types
    #the full date range of observations is included in the sample distribution under the assumptions that
    #more observations will tend to stablize the mean and that major temporal trends in carpooling are unlikely over the fishery duration
    #note that this can still fail if no interviews are available within section-dayType-angler_type...
    ##7/15/22 - dropping stratification to avoid Inf from no-trailer boat anglers and to better match BSS 
    pe$interview_ang_per_vehic <- bind_rows(
      pe$interview |>
        #could re-introduce here: group_by(section)
        summarize(angler_type = "total", anglers_per_vhcl_trlr = sum(angler_count) / sum(vehicle_count), .groups = "drop")
      ,
      pe$interview |>
        filter(angler_type == "boat") |>
        #could re-introduce here: group_by(section)
        summarize(angler_type = "boat", anglers_per_vhcl_trlr = sum(angler_count) / sum(trailer_count), .groups = "drop")
    )
    
    #initially dropped rows of index counts where a given date-section-angler_type is missing from interview data
    #replace with a version using means if no interviews available to assign angler numbers per vehicle/trailer on a given day
    pe$angler_hours_daily_mean <- full_join(
      pe$interview_ang_per_vehic, #coerced above to total/boat
      pe$effort_index |> #already in total/boat
        dplyr::group_by(section, DayType, event_date, angler_type) |>
        dplyr::summarise(count_index_mean = mean(count_index), .groups = "drop")
      ,
      by = c("angler_type")) |>
      group_by(section, DayType, angler_type) |> 
      mutate(
        anglers_per_vhcl_trlr = if_else(
          is.na(anglers_per_vhcl_trlr),
          mean(anglers_per_vhcl_trlr, na.rm=T),
          anglers_per_vhcl_trlr)) |> 
      ungroup() |> 
      left_join(creel$days |> select(event_date, DayL), by = "event_date") |>
      mutate(angler_hours_daily_mean = anglers_per_vhcl_trlr * count_index_mean * DayL) |>
      select(-DayL, -anglers_per_vhcl_trlr, -count_index_mean) |> 
      drop_na(angler_hours_daily_mean) |> 
      arrange(section, event_date)
    
    #now coerce back to angler_type bank/boat (unexpanded)
    if(any(pe$angler_hours_daily_mean$angler_type=="boat")){
      pe$angler_hours_daily_mean <- pe$angler_hours_daily_mean |> 
        pivot_wider(
          names_from = angler_type, 
          values_from = c(angler_hours_daily_mean), 
        ) |>
        mutate(
          boat = tidyr::replace_na(boat, 0),
          bank = total - boat, total = NULL) |> 
        pivot_longer(cols = c(boat, bank), names_to = "angler_type", values_to = "angler_hours_daily_mean") |> 
        #NAs and negatives here reflect inadequate or problematic data barring meaningful inference...
        filter(!is.na(angler_hours_daily_mean), angler_hours_daily_mean >= 0)
    } else {
      pe$angler_hours_daily_mean <- pe$angler_hours_daily_mean |> 
        mutate(angler_type = "bank")
    }

    if(all(creel$effort$tie_in_indicator < 1)) {
      pe$census_TI_expan <- expand_grid(
        section = unique(pe$angler_hours_daily_mean$section), 
        angler_type = unique(pe$angler_hours_daily_mean$angler_type),
        TI_expan_final = 1)
    } else {
      #begin census expansion values object by joining census and index in terms of total & boat
      pe$census_TI_expan <- left_join(
        #census already grouped & summed by event_date, section, tie_in_indicator, count_sequence, and angler_type [bank, boat]
        #but as for interview above, first split and collapse to reassign angler_type as total & boat
        bind_rows(
          pe$effort_census |>
            group_by(section, event_date, count_sequence) |>
            summarize(angler_type = "total", count_census = sum(count_census),  .groups = "drop")
          ,
          pe$effort_census |>
            filter(angler_type == "boat") |>
            group_by(section, event_date, count_sequence) |>
            summarize(angler_type = "boat", count_census = sum(count_census), .groups = "drop")
        ),
        #index counts via interviews for angler-per-vehic; angler_type already total & boat
        #this is very similar to above pe$angler_hours_daily_mean
        #but all count_seqs rather than summarized to daily mean
        #as above, applies mean ang-per-vehic within section-daytype-angtype
        #where interview missing an ang-type or no interviews on that date
        #prevents loss of use of census info b/c of a single day missing interviews...
        full_join(
          pe$interview_ang_per_vehic,
          pe$effort_index,
          by = c("angler_type")
        ) |>
          group_by(section, DayType, angler_type) |> 
          mutate(
            anglers_per_vhcl_trlr = if_else(
              is.na(anglers_per_vhcl_trlr),
              mean(anglers_per_vhcl_trlr, na.rm=T),
              anglers_per_vhcl_trlr)) |> 
          ungroup() |> 
          mutate(count_index = anglers_per_vhcl_trlr * count_index) |>
          select(section, event_date, count_sequence, angler_type, count_index)
        ,
        by = c("section", "event_date", "count_sequence", "angler_type")
      ) |> 
        tidyr::drop_na(count_index)
      
      #now overwrite, coercing angler_type back to bank/boat as above for pe$angler_hours_daily_mean
      #again dropping NAs and negatives as invalid for inferring estimates
      if(any(pe$census_TI_expan$angler_type=="boat")){
        pe$census_TI_expan <- pe$census_TI_expan |> 
          pivot_longer(cols = c(count_census, count_index), names_to = "count_type", values_to = "count") |> 
          pivot_wider(names_from = angler_type, values_from = count) |>
          mutate(bank = total - boat, total = NULL) |> #filter(is.na(boat) | is.na(bank) | bank < 0)
          pivot_longer(cols = c(boat, bank), names_to = "angler_type", values_to = "count") |>
          pivot_wider(names_from = count_type, values_from = count) |> 
          filter(
            !is.na(count_census), !is.na(count_index),
            count_census >= 0, count_index >= 0
          )      
      } else {
        pe$census_TI_expan <- pe$census_TI_expan |>
          mutate(angler_type = "bank")
      }
      
      pe$census_TI_expan <- pe$census_TI_expan |> 
        group_by(section, angler_type) |> 
        summarise(
          across(c(count_census, count_index), sum),
          TI_expan_weighted = count_census / count_index,
          TI_expan_weighted = if_else(TI_expan_weighted == 0, 1, TI_expan_weighted) |> replace_na(1),
          .groups = "drop") |> 
        #and now bring in the external expansion values if any
        left_join(
          lu_input$census_exp |>
            select(cen_exp_meth, angler_type, section, p_TI, TI_expan_indirect = Indirect_TI_Expan),
          by = c("section", "angler_type")) |>
        mutate(
          cen_exp_meth = replace_na(cen_exp_meth, input$census_expansion),
          p_TI = replace_na(p_TI, 1),
          TI_expan_indirect = as.numeric(TI_expan_indirect) |> replace_na(1),
          TI_expan_final = if_else(
            cen_exp_meth == "Direct",
            TI_expan_weighted / p_TI,
            TI_expan_indirect)
        )
    }

    #now multiply mean daily effort in angler_hours by tie-in ratio bias term 
    #aiming for event_date, section, angler_type [total, boat, bank (as total-boat)]
    pe$angler_hours_daily_mean <- left_join(
      pe$angler_hours_daily_mean, 
      pe$census_TI_expan |> select(section, angler_type, TI_expan_final),
      by = c("section", "angler_type")
    ) |>
      mutate(ang_hrs_daily_mean_TI_expan = angler_hours_daily_mean * TI_expan_final)
    
}
  
  #### pe$daily_cpue_catch_est ------------------------

  #aggregate interviews per day per strata of [week/month-weekend/day-section-bank/boat-catch_group]
  #   here including only "mean of ratios" (mean over interviews)
  #   dropping "ratio of the means" (actually sums): sum(fish_count) / sum(angler_hours_total)
  #then multiply by TI-expanded effort estimate
  #dropping any date-section-angler_type-catch_groups for which only interview-based CPUE is available
  #but census-corrected effort estimates are not (various reasons why a day-section-angler_type hours could be NA)
  pe$daily_cpue_catch_est <- pe$interview_and_catch |>
    mutate(cpue_interview = fish_count / angler_hours_total) |>
    group_by(section, time_strata, DayType, event_date, angler_type, catch_group) |>
    summarise(cpue_mor_daily = mean(cpue_interview), .groups = "drop") |> 
    left_join(
      pe$angler_hours_daily_mean, 
      by = c("section", "DayType", "event_date", "angler_type")
    ) |>
    tidyr::drop_na(ang_hrs_daily_mean_TI_expan) |> 
    mutate(catch_estimate = round(cpue_mor_daily * ang_hrs_daily_mean_TI_expan, 3)) |>
    arrange(section, event_date, angler_type, catch_group)
      
  # #degrees of freedom by section and angler type to apply to time strata estimates
#!! should the count of days sampled account for DayType within a week/month-section-angler_type?
  pe$df <- left_join(
    pe$angler_hours_daily_mean, #already has DayType
    creel$days |> select(event_date, time_strata),
    by = "event_date"
    ) |> 
    count(section, time_strata, DayType, angler_type, name = "n_days_samp") |> 
    #count(time_strata, section, angler_type, name = "n_days_samp") |> 
    group_by(section, angler_type) |>
    mutate(df = (min(n_days_samp - 1) + sum(n_days_samp))/2) |> 
    ungroup()
# #!! to recover section-angler_type level only...
#   pe$df |> distinct(section, angler_type, df)

#!!pending above...note still incorrect actual "df = " calc...will update if needed  
  # #used in final set of objects, could move into pe or make inline?
  # ests$degrees_freedom_total <- pe$angler_hours_daily_mean |>
  #   distinct(section, angler_type, n_days = n_days_total) |>
  #   group_by(section, angler_type) |>
  #   summarize(
  #     min_n_days = min(n_days),
  #     sum_n_days = sum(n_days),
  #     degrees_freedom = min_n_days + sum_n_days / 2,
  #     .groups = "drop"
  #   )


  #### pe$est_effort and $est_catch ---------------------
  
  #!!the intersection of section-day-angler_type interview and effort observations may differ...
  #!!opting to derive summarized estimates from the object more likely to encompass more observations
  #!!but could revert to using hours in cpue object, in order to stay fully consistent with catch info?
  #!!     pe$daily_cpue_catch_est |> distinct(section, time_strata, DayType, event_date, angler_type, ang_hrs_daily_mean_TI_expan)

  #calculate mean and variance for section-time_strata-dayType-angler_type
  #this is the finest stratification above individual days 
  #sample size is inherently small for a DayType within week stratification
  # -the var() and sd() functions return NA when passed a length-1 vector (single obs)
  # -variance has limited meaning even when n=3, e.g. if sampling Fri & Sat & Sun
  #BUT sample design itself (and first principles) stratify on DayType
  #such that pooling over weekend/weekday is counter to data collection protocol/design
  #could pool over weeks (and perhaps angler_type) if a better variance is desired over the fishery duration

  pe$est_effort_s_ts_dt_at <- left_join(
    #dates expanded to n-sections * n-angler_types * n-days, previously held as "creel$days_section_angler_type" but only used once
    creel$days |>
      select(time_strata, DayType, event_date, starts_with("open_section")) |> 
      pivot_longer(
        cols = starts_with("open_section"), 
        names_to = "section", 
        values_to = "is_open") |>
      filter(is_open) |> 
      mutate(
        section = as.numeric(gsub("^.*_", "", section)), 
        is_open = NULL,
        angler_type = list(unique(pe$angler_hours_daily_mean$angler_type)) 
      ) |> 
    # unnest(cols = section) |> 
    unnest(angler_type)
    ,
    #estimates of angler_hours possible to calculate for sampled dates-sections-angler_type 
    pe$angler_hours_daily_mean |> select(section, DayType, event_date, angler_type, ang_hrs_daily_mean_TI_expan)
    ,
    by = c("section", "DayType", "event_date", "angler_type")
  ) |>
    group_by(section, time_strata, DayType, angler_type) |>
    summarize(
      n_obs = sum(!is.na(ang_hrs_daily_mean_TI_expan)), #n_days = n(),
      across(
        .cols = c(ang_hrs_daily_mean_TI_expan),
        .fns = list(
          mean = ~mean(.x, na.rm = T),
          var = ~var(.x, na.rm = T)
          ), #sd = ~sd(.x, na.rm = T), med = ~median(.x, na.rm=T),
        .names = "ang_hrs_{.fn}"
      ), .groups = "drop") |> 
    right_join(creel$days_total, by = c("section", "time_strata", "DayType")) |> 
#!!not sure this is correct - could/should recalc df for within-week/month?
    left_join(pe$df |> distinct(section, angler_type, df), by = c("section", "angler_type")) |> 
#!!this carries forward the orig variance eqns but not sure if
#!!1) eqns are correctly implemented or 
#!!2) eqns are meaningful relative to the ang_hrs_var already present from base var() 
#!! i.e., the 3rd term "adjustment coef" in the first case 
#!! acts to reduce the first 2 terms' computed "variance", and is asymptotic to 0 for complete sampling
#!! such that case logic prevents 0 total_effort_var at n_obs==N_days 
    mutate(
      ang_hrs_var = replace_na(ang_hrs_var, 0),
      est = N_days * ang_hrs_mean,
      var = if_else(
        n_obs < N_days,
        (N_days^2) * (ang_hrs_var / n_obs) * (1-(n_obs/N_days)),
        (N_days^2) * (ang_hrs_var / n_obs)
      ),
      l95 = est - qt(1-(0.05/2),df)*(var^0.5),
      u95 = est + qt(1-(0.05/2),df)*(var^0.5)
    ) 
  
  pe$est_catch_s_ts_dt_at <- left_join(
    #dates expanded to n-sections * n-angler_types * n-days, previously held as "creel$days_section_angler_type" but only used once
    creel$days |>
      select(time_strata, DayType, event_date, starts_with("open_section")) |> 
      pivot_longer(
        cols = starts_with("open_section"), 
        names_to = "section", 
        values_to = "is_open") |>
      filter(is_open) |> 
      mutate(
        section = as.numeric(gsub("^.*_", "", section)), 
        is_open = NULL,
        angler_type = list(unique(pe$angler_hours_daily_mean$angler_type)) 
      ) |> 
     # unnest(cols = section) |> 
    unnest(angler_type)
    ,
    pe$daily_cpue_catch_est |> select(section, time_strata, DayType, event_date, angler_type, catch_group, catch_estimate)
    ,
    by = c("section", "time_strata", "DayType", "event_date", "angler_type")
  ) |> 
    group_by(section, time_strata, DayType, angler_type, catch_group) |>
    summarize(
      n_obs = sum(!is.na(catch_estimate)), #n_days = n(),
      across(
        .cols = c(catch_estimate),
        .fns = list(
          mean = ~mean(.x, na.rm = T),
          var = ~var(.x, na.rm = T)
          ), #sd = ~sd(.x, na.rm = T), med = ~median(.x, na.rm=T),
        .names = "catch_est_{.fn}"
      ), .groups = "drop") |> 
    right_join(creel$days_total, by = c("section", "time_strata", "DayType")) |> 
    left_join(pe$df |> distinct(section, angler_type, df), by = c("section", "angler_type")) |> 
    mutate(
      catch_est_var = replace_na(catch_est_var, 0),
      est = N_days * catch_est_mean,
      var = if_else(
        n_obs < N_days,
        (N_days^2) * (catch_est_var / n_obs) * (1-(n_obs/N_days)),
        (N_days^2) * (catch_est_var / n_obs)
      ),
      l95 = est - qt(1-(0.05/2),df)*(var^0.5),
      u95 = est + qt(1-(0.05/2),df)*(var^0.5)
    ) |> 
    drop_na(catch_group)


}) |> 
  bindEvent(input$estimate)
```


Data from DWG
============================================

Days
--------------------------------------------

### Potential days of fishing by time strata, section and day type

```{r gt_creel_days}
render_gt({ 
  if(!is.null(creel$days_total)) {
    creel$days_total |> 
      pivot_wider(names_from = c(section, DayType), values_from = N_days) |> 
      gt(rowname_col = "time_strata")
  }
})
```


Effort
--------------------------------------------

### Index (tie_in: 0) and Census (tie_in: 1) sampling to date

```{r gt_creel_effort}
render_gt({ 
  if(!is.null(creel$event)) {
    creel$effort |> 
      distinct(water_body, section, location, event_date, tie_in_indicator, count_sequence) |> 
      count(water_body, section, location, tie_in_indicator) |> 
      unite("water_body_section", water_body, section) |> 
      arrange(water_body_section, desc(tie_in_indicator)) |> 
      gt(groupname_col = "water_body_section")
  }
})
```

Interview
--------------------------------------------

### Number of interviews by analysis section

```{r gt_creel_interview}
render_gt({ 
  if(!is.null(creel$interview)) {
    creel$interview |> 
      count(water_body, section, location) |> 
      gt()
  }
})
```

Catch
--------------------------------------------

### Reported encounters in interviews to date

```{r gt_creel_catch}
render_gt({
  if(!is.null(creel$catch)) {
    creel$catch |> 
      group_by(catch_group) |> 
      summarise(fish_count = sum(fish_count), .groups = "drop") |> 
      gt()
  }
})
```


Observations
============================================

Index
--------------------------------------------

### Index counts

```{r gg_effort_index_col}
renderPlot({
  if(!is.null(pe$effort_index)) {
    pe$effort_index |>
      mutate(count_sequence = factor(count_sequence)) |> 
      ggplot(aes(event_date, count_index, fill = count_sequence, color = count_sequence)) +
      #geom_point() + geom_text(aes(label = count_index), nudge_y = 1, check_overlap = T) +
      geom_col(position = position_dodge(width = 0.7)) +
      scale_x_date("", date_breaks = "3 days", date_labels =  "%m-%d") + scale_y_continuous("") +
      scale_color_brewer(palette = "Set2", aesthetics = c("color", "fill")) +
      facet_wrap(~section + count_type + angler_type, scales = "free", ncol = 1, labeller = label_wrap_gen(multi_line = F))
  }
})
```

Census
--------------------------------------------

### Census counts

```{r gg_effort_census_index_scatter}
renderPlot({
  if(!is.null(pe$census_TI_expan)) {
    pe$census_TI_expan |> 
      ggplot(aes(count_index, count_census, color = angler_type)) +
      geom_abline(slope = 1, intercept = 0, linetype = 2) +
      scale_x_continuous(limits = c(0, NA)) +
      scale_y_continuous(limits = c(0, NA)) +
      geom_point() +
      geom_smooth(method = "lm", se = FALSE) +
      facet_wrap(~paste("Section:",section), labeller = label_wrap_gen(multi_line = F))
  }
})
```


Interview
--------------------------------------------

### Interview

```{r gg_interview_dot_smooth}
renderPlot({
  if(!is.null(pe$interview)) { 
    pe$interview |>
      ggplot(aes(event_date, angler_hours_total, color = angler_type, fill = angler_type)) +
      geom_jitter(width = 0.1, alpha = 0.7) +
      geom_smooth(
        data = pe$interview |> group_by(section, angler_type, event_date) |> 
          summarise(angler_hours_total = median(angler_hours_total), .groups = "drop"),
        formula = y ~ x, method = "loess", se = F
      ) +
      scale_x_date() +
      facet_wrap(~section + angler_type, scales = "free_x", ncol = 2, labeller = label_wrap_gen(multi_line = F)) 
  }
})
```

Catch
--------------------------------------------

### Catch

```{r pe$interview_and_catch_gt}
render_gt({
  if(!is.null(pe$interview_and_catch)) {
    pe$interview_and_catch |>
      group_by(section, time_strata, DayType, event_date, angler_type, catch_group) |> 
      summarise(fish_count = sum(fish_count), .groups = "drop") |> 
      filter(fish_count > 0) |>
      pivot_wider(names_from = catch_group, values_from = fish_count, names_sort = T) |> 
      mutate(section = paste("section", section)) |> 
      gt(groupname_col = "section", rowname_col = "event_date") |> 
      tab_options(container.overflow.x = T, container.overflow.y = T) |> 
      summary_rows(groups = TRUE, 
                   columns = -c(section, time_strata, DayType, event_date, angler_type),
                   fns = list(sum = ~sum(., na.rm = T)), 
                   formatter = fmt_integer) |> 
      sub_missing() 
    
    # pivot_wider(names_from = angler_type, values_from = fish_count) |> 
    # gt(groupname_col = "catch_group", rowname_col = "event_date") |> 
    # sub_missing(c("bank", "boat"))
  }
})

```

Effort
============================================

Table
--------------------------------------------

### Total angler hours, sum over temporal strata {data-height=300}

```{r gt_effort_totals}
render_gt({
  if(!is.null(pe$est_effort_s_ts_dt_at)){
    #by angler_type, summed over weekday/weekend and week/month
    pe$est_effort_s_ts_dt_at |>
      group_by(section, angler_type) |>
      summarise(across(c(est, var), ~round(sum(., na.rm = T))), .groups = "drop") |> 
      pivot_wider(names_from = angler_type, values_from = c(est, var)) |> 
      select(section, contains("bank"), contains("boat")) |> 
      gt() |> 
      fmt_number(columns = -section, decimals = 0) |> 
      gt::summary_rows(columns = contains("est"), fns = list(sum = ~round(sum(., na.rm = T),0)), decimals = 0)
  }
})
```

Plot1
--------------------------------------------

### Census-adjusted daily mean angler hours

```{r gg_angler_hours_daily_mean}
renderPlot({
  if(!is.null(pe$est_effort_s_ts_dt_at)){
    pe$angler_hours_daily_mean |> 
      ggplot(aes(event_date, ang_hrs_daily_mean_TI_expan, fill = angler_type)) +
      geom_col(position = position_stack()) +
      scale_x_date("", date_breaks = "3 day", guide = guide_axis(n.dodge = 2)) +
      scale_y_continuous("Hours") +
      facet_wrap(~section + DayType, scales = "fixed", labeller = label_wrap_gen(multi_line = F), ncol = 2) +
      labs(
        title = "Daily angler hours, sampled days",
        subtitle = "Day length * (Census-adjusted and interview-informed index counts)")
  }
})
```

Plot2
--------------------------------------------

### Angler hours by section-time_strata-DayType-angler_type 

```{r gg_angler_hours_est_ribbons}
renderPlot({
  if(!is.null(pe$est_effort_s_ts_dt_at)){
    pe$est_effort_s_ts_dt_at |> 
      ggplot(aes(time_strata, fill = angler_type, color = angler_type)) +
      geom_ribbon(aes(y = est, ymin = l95, ymax = u95), alpha = 0.4) +
      geom_line(aes(y = est)) +
      scale_y_continuous("Hours") +
      facet_wrap(~section + DayType, scales = "fixed", labeller = label_wrap_gen(multi_line = F), ncol = 2) +
      labs(
        title = "Estimated angler hours by time strata",
        subtitle = "Section-time-strata-DayType-angler-type means * N-days")
  }
})
```

Plot3
--------------------------------------------

### Cumulative angler hours by day and angler type

```{r gg_cumulative_ang_hours}
renderPlot({
  if(!is.null(pe$est_effort_s_ts_dt_at)){
    pe$est_effort_s_ts_dt_at |> 
      drop_na(est) |> 
      group_by(DayType, angler_type) |> 
      mutate(effort_cml = cumsum(est)) |> 
      ungroup() |> 
      ggplot(aes(time_strata, effort_cml, color = angler_type)) +
      geom_line() +
      scale_y_continuous("Hours") +
      facet_wrap(~section + DayType, scales = "fixed", labeller = label_wrap_gen(multi_line = F), ncol = 2) +
      labs(
        title = "Cumulative estimated angler hours, strata-means to complete days",
        subtitle = "Section-time-strata-DayType-angler-type means * N-days")
  }
})
```

Catch totals
============================================

row {data-height=1000}
--------------------------------------------

### Total across all sections & strata

```{r gt_total_by_catch_group}
render_gt({
  if(!is.null(pe$est_catch_s_ts_dt_at)){
    pe$est_catch_s_ts_dt_at |> 
      group_by(catch_group) |> 
      summarise(est = round(sum(est), 1), .groups = "drop") |> 
      gt() |> 
      fmt_number(columns = where(is.numeric), decimals = 1)
  }
})

```


Chinook
============================================

row {data-height=1000}
--------------------------------------------

### Marked

```{r chin_adult_ad}
render_gt({
  if(!is.null(pe$est_catch_s_ts_dt_at)){
    gt_catch("Chinook_Adult_AD")
  }    
})
```

### Unmarked

```{r chin_adult_um}
render_gt({
  if(!is.null(pe$est_catch_s_ts_dt_at)){
    gt_catch("Chinook_Adult_UM")
  }    
})
```

### Jack

```{r chin_jacks}
render_gt({
  if(!is.null(pe$est_catch_s_ts_dt_at)){
    gt_catch("Chinook_Jack|Chinook_Smolt|Chinook_Unknown")
  }    
})
```

Coho
============================================

row {data-height=1000}
--------------------------------------------

### Marked

```{r coho_adult_ad}
render_gt({
  if(!is.null(pe$est_catch_s_ts_dt_at)){
    gt_catch("Coho_Adult_AD")
  }    
})
```

### Unmarked

```{r coho_adult_um}
render_gt({
  if(!is.null(pe$est_catch_s_ts_dt_at)){
    gt_catch("Coho_Adult_UM")
  }    
})
```

### Jack

```{r coho_jacks}
render_gt({
  if(!is.null(pe$est_catch_s_ts_dt_at)){
    gt_catch("Coho_Jack|Coho_Smolt|Coho_Unknown")
  }    
})
```

Steelhead
============================================

row {data-height=1000}
--------------------------------------------

### Adult

```{r sthd_adult}
render_gt({
  if(!is.null(pe$est_catch_s_ts_dt_at)){
    gt_catch("Steelhead_Adult")
  }    
})
```

### Juvenile

```{r sthd_other}
render_gt({
  if(!is.null(pe$est_catch_s_ts_dt_at)){
    gt_catch("Steelhead_Jack|steelhead_Smolt|Steelhead_Unknown")
  }    
})
```


Sockeye
============================================

row {data-height=1000}
--------------------------------------------

### Marked

```{r sock_adult_ad}
render_gt({
  if(!is.null(pe$est_catch_s_ts_dt_at)){
    gt_catch("Sockeye_Adult_AD")
  }    
})
```

### Unmarked

```{r sock_adult_um}
render_gt({
  if(!is.null(pe$est_catch_s_ts_dt_at)){
    gt_catch("Sockeye_Adult_UM")
  }    
})
```

### Jack/other

```{r sock_jacks}
render_gt({
  if(!is.null(pe$est_catch_s_ts_dt_at)){
    gt_catch("Sockeye_Jack|Sockeye_Smolt|Sockeye_Unknown")
  }    
})
```


Other encounters
============================================

row {data-height=1000}
--------------------------------------------

### Others

```{r others}
render_gt({
  if(!is.null(pe$est_catch_s_ts_dt_at)){
    gt_catch(cg_string = "Chinook|Coho|Steelhead|Sockeye", negate = T)  
  } 
})
```


About
============================================

Anything to cite/link to?

WDFW gathers fishing information from anglers around the state: <a href="https://wdfw.wa.gov/fishing/reports/creel"> https://wdfw.wa.gov/fishing/reports/creel</a>
